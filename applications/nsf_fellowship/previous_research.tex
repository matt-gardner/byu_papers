\documentclass[onecolumn, 12pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{natbib}

\setlength{\textheight}{9in}
\setlength{\bibsep}{0in}

\begin{document}

\pagestyle{empty}

\begin{center}
  \textbf{Previous Research Experience}\\
  Matthew Gardner
\end{center}
\vspace{-4mm}

Brigham Young University is somewhat unique among universities in that it gives
undergraduates superb opportunities to participate in research, even working
very closely with professors.  During my second year of college, I began
working in the Applied Machine Learning lab at BYU, headed by Dr. Kevin Seppi,
and I was one of only three students that he advised at the time.  For the next
two and a half years, I worked on several aspects of essentially the same
problem---how to effectively solve long-running optimization problems with a
large cluster of hundreds or thousands of machines.

This project started when a physics professor approached my advisor seeking
help solving a crystallography problem.  As a new student in the lab, I
accepted the problem mostly in an auxiliary role, writing some code and running
experiments for Dr. Campbell, the physics professor.  We used particle swarm
optimization (PSO), a bio-inspired optimization algorithm similar to
evolutionary algorithms, to try to fit to data his model of what was happening
in a particular kind of crystal.  After spending a lot of time seeing the
output of the algorithm, I also helped to interpret the results.  I discovered
that we had too many free parameters in our model and helped Dr.  Campbell to
make progress in solving his problem.  Because of our help in solving his
problem, Dr. Campbell has made what he considers a breakthrough in
understanding these crystals and plans to submit his work for publication soon.

The crystallography problem we were solving required the evaluation of a
function that took several minutes to run.  As each particle in the algorithm
must evaluate the function at every iteration, we used several hundred
processors on BYU's supercomputer in our optimization of the function.  This
led us to ask some interesting questions about how we could best use those
processors to solve the problem.  We first experimented with the topology of
the algorithm, or how the particles in the swarm communicate with each other.
I did much of the experimentation, though I was directed by a graduate student
in the lab, who eventually wrote the paper that we
published~\cite{mcnabb-2009-large-particle-swarms}.  We saw some improvements
in the performance of the algorithm with our new topologies, though we also
noticed that increasing the number of particles with additional processors had
diminishing returns.

To solve this problem of diminishing returns, my advisor proposed applying the
idea of speculative decomposition in the parallelization literature to PSO.
Instead of increasing the number of particles in the swarm, additional
processors could be used to evaluate possible next-iteration positions of each
particle in parallel.  I thought it was an interesting idea, and I made it my
honors thesis.  I took the idea and discovered how it could actually be done,
performing the evaluations needed and piecing the results back together from
messages sent between processors so that speculative evaulation could be done
in a distributed PSO environment with only a single round of message passing
per iteration.  After implementing the technique, I ran many experiments to
compare our methods to other means of parallelizing the algorithm.  Once I was
satisfied that my results were significant, I wrote, submitted, and
successfully defended my honors thesis.  I also modified the paper to fit in
the aims and page requirements of Parallel Problem Solving from Nature, a
well-respected parallel optimization conference.  I submitted my thesis there
and it was accepted~\cite{gardner-2010-speculative-evaluation-in-pso}.

In implementing speculative evaluation, however, I noticed a number of ways the
method could potentially be improved.  Even though my honors thesis was
completed I continued my investigation, as I thought there was more that could
be done.  I found that a large number of the speculative evaluations we
performed were wasted, and that the algorithm could be improved by replacing
useless evaluations with better evaluations, such as speculating several
iterations ahead on paths that particles were likely to take.  I significantly
expanded the work of my honors thesis, preparing to submit it to a journal on
optimization.  After a rejection of a smaller version from a conference, I
became quite discouraged, wanting to give up and move on to something new.  My
advisor insisted I persevere, saying it was good preparation for a PhD, and so
I continued.  After running more experiments to strengthen the paper, we
submitted it to the journal Swarm Intelligence, where it is currently under
review~\cite{gardner-2010-speculative-approach-to-parallelization-pso}.

While I have since move on to other interests, this project was my first
experience performing research and dealing with the submission process at
conferences.  I learned a lot about what research is, the perseverance required
in getting a PhD, and how much I love exploring new topics and extending the
limits of what we can do, as well as helping researchers in other areas to
solve their problems.  This project set the stage for my masters thesis and for
my future work as a PhD student.  

Along with the research I performed as an undergraduate research assistant, two
other projects are worth noting here.  The first is my internship at Google in
the summer of 2009.  I tackled the problem of how to determine the
effectiveness of TV advertisements given only a list of when and on what
channels the ads played, along with a list of phone calls received in response
to the ads.  This was complicated because often ads play at the same time on
different channels, and they use the same phone number.  I developed and
implemented a probabilistic model that assigned phone calls to ads that were
played.  The model learned a response rate for each channel the ads played on
and helped advertisers know where to most effectively spend their money.  Once
advertisers saw that following the recommendations of my model gave them better
response rates, they spent more on advertising with Google.

The other research area that I have pursued is topic modeling, a method for
capturing the semantics of a document collection, and that is the area of my
masters thesis.  As an undergraduate, I implemented a topic model and used it
to analyze a novel dataset of religious discourses, which I presented at an
international conference held at
BYU~\cite{gardner-2010-general-conference-topics}.  As a graduate student, I
have been investigating how to use topic models to get specific, corpus-wide
information out of large collections of documents.  My initial endeavors have
involved building a tool to browse the output of a topic model in new
ways~\cite{gardner-2010-topic-browser}.  As other departments at BYU have seen
the work that I and others have done with our topic browser, they have been
very interested in using our tool.  Three separate groups at BYU have asked to
use our browser to analyze data with purposes ranging from finding problem
areas in teaching from free-form student ratings of classes and professors to
searching for public health information in Twitter feeds.  My future plans with
topic modeling are presented in my research proposal.

\footnotesize
\bibliographystyle{plain}
\renewcommand\bibsection{\noindent \small\textbf{Publications}\vspace{-2mm}\footnotesize}
\bibliography{../../../bib/aml/bib}

\end{document}

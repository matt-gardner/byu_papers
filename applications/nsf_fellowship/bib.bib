@article{czirr-2000-neutron-spectrometry,
    author = "J. Bart Czirr and David B. Merrill and David Buehler and Thomas K. McKnight and James L. Carroll and Thomas Abbott and Eva Wilcox",
    title = "Capture-gated Neutron Spectrometry",
    journal = "Nuclear Instruments and Methods A",
    year = "2000",
    date = "June 2000",
    }

@inProceedings{carrol-l2001-meory-guided,
    AUTHOR = "James L. Carroll and Todd Peterson and Nancy Owens",
    TITLE = "Memory-Guided Exploration in Reinforcement Learning",
    BOOKTITLE = "In IJCNN2001",
    ADDRESS = "Washington, D.C.",
    YEAR = "2001",
    ANNOTE = "This is our paper which discusses the convergence issues involved with memory-guided exploration"}


@inProceedings{carroll-2002-dynamic-transfer,
    AUTHOR = "James L. Carroll and Todd Peterson",
    TITLE = "Fixed vs. Dynamic Sub-transfer in Reinforcement Learning",
    BOOKTITLE = "ICMLA",
    ADDRESS = "Las Vegas Nevada, USA",
    YEAR = "2002",
    PUBLISHER = "CSREA Press",
    ANNOTE = "This paper shows that dynamic sub-transfer outperforms fixed sub-transfer, and that it has better convergence properties",
}

@inProceedings{carroll-2003-RL-task-clustering,
    AUTHOR = "James L. Carroll and Todd Peterson and Kevin Seppi",
    TITLE = "Reinforcement Learning Task Clustering (RLTC)",
    BOOKTITLE = "International Conference on Machine Learning and Applications (ICMLA)",
    ADDRESS = "LA California USA",
    YEAR = "2003",
    ANNOTE = "This paper shows how task clustering can be performed in reinforcement learning, and how transfer could be accomplished from a given cluster.",
}

@INPROCEEDINGS{carroll-2004-task-localization,
 author = "James L. Carroll and Kevin Seppi",
 title = "A Bayesian Technique for Task Localization in Multiple Goal Markov Decision Processes",
 booktitle = "International Conference on Machine Learning and Applications, (ICMLA)",
 year = "2004",
}

@INPROCEEDINGS{carroll-2005-task-similarity,
 author = "James L. Carroll and Kevin Seppi",
 title = "Task Similarity Measures for Transfer in Reinforcement Learning Task Libraries",
 booktitle = "International Joint Conference on Neural Networks, (IJCNN)",
 year = "2005",
}

@mastersthesis{carroll-2005-towards-RL-task-library,
 author = "James L. Carroll",
 title = "Task Localization, Similarity, and Transfer; Towards a Reinforcement Learning Task Library System",
 school="Brigham Young University, Department of Computer Science",
 address = "Provo Utah",
 year = "2005",
}

@article{carroll-2007-bayesian-CMAC-for-high-assurance,
    author = "James L. Carroll and Christopher K. Monson and Kevin D. Seppi",
    title = "A Bayesian CMAC for High Assurance Supervised Learning",
    journal = "Applications of Neural Networks in High-Assurance Systems, NASA-IJCNN Workshop",
    year = "2007",
    }

@article{carroll-2007-NFL-and-bayesian-optimality,
  author = "James L. Carroll and Kevin D. Seppi",
  title = "No-Free-Lunch and Bayesian Optimality",
  journal = "Meta-Learning IJCNN Workshop",
  year = "2007",
  abstract = "We take a Bayesian approach to the issues of bias, meta bias, transfer, overfit, and No-Free-Lunch in the context of supervised learning. If we accept certain relationships between the function class, on training set data, and off training set data, then a graphical model can be created that represents the supervised learning problem. This graphical model dictates a specific algorithm which will be the “optimal” approach to learning the parameters of any given function representation given the variable relationships. Thus, there is an optimal technique for supervised learning. We reconcile this idea of an optimal technique with the ideas of No-Free-Lunch and show how these ideas relate to the concepts of meta and transfer learning through hierarchical versions of the graphical model."
}

@article{carroll-2007-modeling-annotation-process,
    author = "James L. Carroll and Robbie Haertel and Peter McClanahan and Eric Ringger and Kevin Seppi",
    title = "Modeling the Annotation Process for Ancient Corpus Creation",
    journal = "Proceedings of the International Conference of Electronic Corpora of Ancient Languages (ECAL), or Chatressar 2007",
    address = "Prague",
    date = "November 2007",
    year = "2007",
}

@article{carroll-2008-explicit-utility,
    author = "James L. Carroll and Neil Toronto and Robbie Haertel and Kevin Seppi",
    title = "Explicit Utility in Supervised Learning",
    journal = "NIPS Workshop on Cost-Sensitive Machine Learning",
    year = "2008",
    address = "Whistler, British Columbia, Canada",
    abstract = "We use a graphical model of the supervised learning problem to explore the theoretical effect of utility on supervised learning,
        No-Free-Lunch, sample complexity, and active learning. There are two sources of utility that can affect the above problems: utility that comes
        from end use and utility that comes from sample costs. We explore which parts of these problems depend on utility, and which parts are
        utility free. Further, we propose a novel interpretation of the No-Free-Lunch theorems that is independent of utility. We propose that sample
        complexity should be redefined in terms of expected sample costs to achieve a given threshold on expected end use effectiveness (which
        would be defined in terms of end use utility). finally, we explore the effects of the sample cost function and the end use utility function
        on active learning techniques both theoretically (through the optimal active learning equations) and through several examples including a
        synthetic data set and a real life part of speech tagging scenario.",
}

@phdthesis{carroll-2010-bayesian-decision-theoretical-approach,
   author = "James L. Carroll",
   title = "A Bayesian Decision Theoretical Approach to Supervised Learning, Selective Sampling, and Empirical Function Optimization",
   month = "March",
   year = "2010",
   school = "Brigham Young University",
   url = "http://james.jlcarroll.net/publications/",
   annote = "A Bayesian model of Function Learning, creating a unified model that is capable of answering many theory questions.",
}


@inproceedings{gardner-2010-general-conference-topics,
	author = {M. Gardner and E. Ringger},
	title = {{Automatic Topic Discovery in 100 Years of General Conference
		Talks}},
	booktitle = {50th Anniversary BYU Studies Symposium},
	year = {2010},
	abstract = {},
	annote = {},
}

@article{gardner-2010-speculative-evaluation-in-pso,
	author = {M. Gardner and A. McNabb and K. Seppi},
	title = {{Speculative Evaluation in Particle Swarm Optimization}},
	journal = {Parallel Problem Solving from Nature XI},
	pages = {61--70},
	year = {2010},
	publisher = {Springer},
	abstract = {Particle swarm optimization (PSO) has previously been
		parallelized only by adding more particles to the swarm or by
			parallelizing the evaluation of the objective function.  However,
		some functions are more efficiently optimized with more iterations and
			fewer particles.  Accordingly, we take inspiration from speculative
			execution performed in modern processors and propose speculative
			evaluation in PSO (SEPSO).  Future positions of the particles are
			speculated and evaluated in parallel with current positions,
		performing two iterations of PSO at once.  We also propose another way
			of making use of these speculative particles, keeping the best
			position found instead of the position that PSO actually would have
			taken.  We show that for a number of functions, speculative
			evaluation gives dramatic improvements over adding additional
			particles to the swarm.
	},
	annote = {},
}

@article{gardner-2010-speculative-approach-to-parallelization-pso,
	author = {M. Gardner and A. McNabb and K. Seppi},
	title = {{A Speculative Approach to Parallelization in Particle Swarm
		Optimization}},
	journal = {Swarm Intelligence},
	year = {Currently under review},
	publisher = {Springer},
	abstract = {Particle swarm optimization (PSO) has previously been
		parallelized only by adding more particles to the swarm or by
			parallelizing the evaluation of the objective function.  However,
		some functions are more efficiently optimized with more iterations and
			fewer particles.  Accordingly, we take inspiration from speculative
			execution performed in modern processors and propose speculative
			evaluation in PSO (SEPSO).  Future positions of the particles are
			speculated and evaluated in parallel with current positions,
		performing two iterations of PSO at once.  We also propose another way
			of making use of these speculative particles, keeping the best
			position found instead of the position that PSO actually would have
			taken.  We show that for a number of functions, speculative
			evaluation gives dramatic improvements over adding additional
			particles to the swarm.
	},
	annote = {},
}

@inproceedings{gardner-2010-topic-browser,
	author = {M. Gardner et al},
	title = {{The Topic Browser: An Interactive Tool for Browsing Topic
		Models}},
	booktitle = {NIPS Workshop on Challenges of Data Visualization},
	year = {2010},
	abstract = {},
	annote = {},
}

@INPROCEEDINGS{haertel-2008-return-on-investment,
 author = {Robbie A. Haertel and Kevin D. Seppi and Eric K. Ringger and James L. Carroll},
 title = {Return on Investment for Active Learning},
 booktitle = {NIPS Workshop on Cost-Sensitive Machine Learning},
 year = {2008},
 address = {Whistler, British Columbia, Canada},
 abstract = "Active Learning (AL) can be defined as a selectively supervised learning protocol intended to present those data to an oracle for labeling which will be most enlight-ening for machine learning. While AL traditionally accounts for the value of the information obtained, it often ignores the cost of obtaining the information thus causing it to perform sub-optimally with respect to total cost. We present a frame-work for AL that accounts for this cost and discuss optimality and tractability in this framework. Using this framework we motivate Return On Investment (ROI), a practical, cost-sensitive heuristic that can be used to convert existing algorithms into cost-conscious active learners. We demonstrate the validity of ROI in a simulated AL part-of-speech tagging task on the Penn Treebank in which ROI achieves as high as a 73% reduction in hourly cost over random selection.",
}

@INPROCEEDINGS{haertel-2009-assessing-costs,
    author = {Robbie Haertel and Eric Ringger and Kevin Seppi and James Carroll and Peter McClanahan},
    title = {Assessing the Costs of Sampling Methods in Active Learning for Annotation},
    booktitle = {Proceedings of the Conference of the Association of Computational Linguistics (ACL-NAACL: HLT 2008)},
    year = {2008},
 abstract = " Traditional Active Learning (AL) techniques assume that the annotation of each datum costs the same. This is not the case when annotating sequences; some sequences will take longer than others. We show that the AL technique which performs best depends on how cost is measured. Applying an hourly cost model based on the results of an annotation user study, we approximate the amount of time necessary to annotate a given sentence. This model allows us to evaluate the effectiveness of AL sampling methods in terms of time spent in annotation. We acheive a 77% reduction in hours from a random baseline to achieve 96.5% tag accuracy on the Penn Treebank. More significantly, we make the case for measuring cost in assessing AL methods.",
}

@INPROCEEDINGS{heal-2007-computational-perspective-corpus-development,
 author = "Kristian Heal and Carl Griffin and Eric Ringger and Peter McClanahan and James Carroll and Joshua Heaton and et al.",
 title = "A Computational Perspective on Syriac Corpus Development and Annotation",
 booktitle = "a Presentation Given at the XIXth Congress of the International Organization for the Study of the Old Testament (IOSOT), and the International Syriac Language Project (ISLP)",
 year = "2007",
 month = "July",
 date = "July 2007",
}

@inproceedings{mcnabb-2007-mapreduce-pso,
    title={{MRPSO}: {MapReduce} Particle Swarm Optimization},
    author={Andrew W. McNabb and Christopher K. Monson and Kevin D. Seppi},
    booktitle={Proceedings of the 9th annual conference on Genetic and evolutionary computation},
    year={2007},
    month=jul,
    volume={},
    number={},
    pages={177},
    keywords={parallel programming, particle swarm optimisation},
}
doi={10.1109/CEC.2007.4424448},

@inproceedings{mcnabb-2007-parallel-pso-using-mapreduce,
    title={Parallel {PSO} using {MapReduce}},
    author={Andrew W. McNabb and Christopher K. Monson and Kevin D. Seppi},
    booktitle={Proceedings of the IEEE Congress on Evolutionary Computation},
    year={2007},
    month=sep,
    volume={},
    number={},
    pages={7--14},
    keywords={parallel programming, particle swarm optimisation},
}
doi={10.1109/CEC.2007.4424448},

@inproceedings{mcnabb-2009-large-particle-swarms,
    title={{An Exploration of Topologies and Communication in Large Particle Swarms}},
    author={A. McNabb and M. Gardner and K. Seppi},
    booktitle={Congress on Evolutionary Computation},
    year={2009},
    month=may,
    volume={},
    number={},
    pages={712--719},
}
doi={10.1109/CEC.2007.4424448},

@inproceedings{monson-2005-origin-seeking-bias,
    author="Christopher K. Monson and Kevin D. Seppi",
    title="Exposing Origin-Seeking Bias in PSO",
	booktitle="Proceedings of the Genetic and Evolutionary Computation
		Conference ({GECCO} 2005)",
    address="Washington, D.C.",
    pages="241--248",
    volume="1",
    year="2005",
	abstract={ We discuss testing methods for exposing origin-seeking biasin
		PSO motion algorithms. The strategy of resizing the ini-tialization
			space, proposed by Gehlhaar and Fogel and madepopular in the PSO
			context by Angeline, is shown to be in-sufficiently general for
			revealing an algorithm’s tendency tofocus its efforts on regions at
			or near the origin. An alterna-tive testing method is proposed that
			reveals problems withPSO motion algorithms that are not visible
			when merelyresizing the initialization space.
	},
	annote={},
}

@inproceedings{monson-2007-UFO,
    author="Christopher K. Monson and Kevin D. Seppi and James L. Carroll",
    title="A Utile Function Optimizer",
    booktitle="The Proceedings of the IEEE Congress on Evolutionary Computation (CEC)",
    year=2007,
    month="Sept",
    publisher="IEEE Press",
    annote = "EVSI can be used to determine optimal sample locations similar to active learning in supervised
        learning to guide a Bayesian optimization algorithm.",
}

@inProceedings{peterson-2001-automatic-shaping,
    AUTHOR = "Todd Peterson and Nancy Owens and James L. Carroll",
    TITLE = "Automated Shaping as Applied to Robot Navigation",
    BOOKTITLE = "IEEE International Conference on Robotics and
            Automation",
    ADDRESS = "Korea",
    YEAR = "2001",
    ANNOTE = "This paper is the first paper to discuss memory-guided exploration"
}

@INPROCEEDINGS{ringger-2007-AL-for-POS-tagging,
 author = "Eric Ringger and Peter McClanahan and Robbie Haertel and George Busby and Marc Carmen and James Carroll and Kevin Seppi and Deryle Lonsdale",
 title = "Active Learning for Part-of-Speech Tagging: Accelerating Corpus Annotation",
 booktitle = "Proceedings of the ACL Linguistic Annotation Workshop (LAW)",
 publisher = "Association for Computational Linguistics",
 address = "Prague, Czech Republic",
 year = "2007",
 month = "June",
 date = "June 2007",
 page = "101-108",
 abstract = "In the construction of a part-of-speech an- notated corpus, we are constrained by a fixed budget. A fully annotated corpus is required, but we can afford to label only a subset. We train a Maximum Entropy Mar- kov Model tagger from a labeled subset and automatically tag the remainder. This paper addresses the question of where to focus our manual tagging efforts in order to deliver an annotation of highest quality. In this context, we find that active learning is always helpful. We focus on Query by Un- certainty (QBU) and Query by Committee (QBC) and report on experiments with sev- eral baselines and new variations of QBC and QBU, inspired by weaknesses particu- lar to their use in this application. Experi- ments on English prose and poetry test these approaches and evaluate their robust- ness. The results allow us to make recom- mendations for both types of text and raise questions that will lead to further inquiry.",
}

@INPROCEEDINGS{ringger-2008-user-study,
 author = {Eric Ringger and Marc Carmen and Robbie Haertel and Kevin Seppi and Deryle Lonsdale and Peter McClanahan and James Carroll and Noel Ellison},
 title = {Assessing the Costs of Machine-Assisted Corpus Annotation through a User Study},
 booktitle = {The Proceedings of the Language Resources and Evaluation Conference (LREC)},
 year = {2008},
 date = {May 2008},
 address = "Morocco",
 abstract = "Fixed, limited budgets often constrain the amount of expert annotation that can go into the construction of annotated corpora.
Estimating the cost of annotation is the first step toward using annotation resources wisely. We present here a study of the cost of
annotation. This study includes the participation of annotators at various skill levels and with varying backgrounds. Conducted over
the web, the study consists of tests that simulate machine-assisted pre-annotation, requiring correction by the annotator rather than
annotation from scratch. The study also includes tests representative of an annotation scenario involving Active Learning as it
progresses from a naïve model to a knowledgeable model; in particular, annotators encounter pre-annotation of varying degrees of
accuracy. The annotation interface lists tags considered likely by the annotation model in preference to other tags. We present the
experimental parameters of the study and report both descriptive and inferential statistics on the results of the study. We conclude with
a model for estimating the hourly cost of annotation for annotators of various skill levels. We also present models for two granularities
of annotation: sentence at a time and word at a time.",
}
@article{blei-2003-latent-dirichlet-allocation,
	title = {Latent Dirichlet Allocation},
	volume = {3},
	doi = {10.1162/jmlr.2003.3.4-5.993},
	journal = {Journal of Machine Learning Research},
	author = {{D.} Blei and {A.} Ng and {M.} Jordan},
	year = {2003},
	pages = {993--1022},
	abstract = { We describe latent Dirichlet allocation (LDA), a generative
		probabilistic model for collections of discrete data such as text
			corpora. LDA is a three-level hierarchical Bayesian model, in which
			each item of a collection is modeled as a finite mixture over an
			underlying set of topics. Each topic is, in turn, modeled as an
			infinite mixture over an underlying set of topic probabilities. In
			the context of text modeling, the topic probabilities provide an
			explicit representation of a document. We present efficient
			approximate inference techniques based on variational methods and
			an EM algorithm for empirical Bayes parameter estimation. We report
			results in document modeling, text classification, and
			collaborative filtering, comparing to a mixture of unigrams model
			and the probabilistic LSI model.
	},
	annote = { Matt: The paper that introduced LDA. 
   	},
}

@article{blei-2008-supervised-topic-models,
	title={{Supervised topic models}},
	author={Blei, D. and McAuliffe, J.},
	journal={Advances in Neural Information Processing Systems},
	volume={20},
	pages={121--128},
	year={2008},
	abstract = { We introduce supervised latent Dirichlet allocation (sLDA), a
		statistical model of labelled documents. The model accommodates a
			variety of response types. We derive an approximate
			maximum-likelihood procedure for parameter estimation, which relies
			on variational methods to handle intractable posterior
			expectations. Prediction problems motivate this research: we use
			the fitted model to predict response values for new documents. We
			test sLDA on two real-world problems: movie ratings predicted from
			reviews, and the political tone of amendments in the U.S. Senate
			based on the amendment text. We illustrate the benefits of sLDA
			versus modern regularized regression, as well as versus an
			unsupervised LDA analysis followed by a separate regression.  
	},
	annote = { Matt: While this paper was not the first (I don't think) to deal
		with labels or tags on documents in LDA, it is an important one.  They
			add a label node to LDA inside each document, which they assume is
			general exponentially distributed (Gaussian, Poisson).  Thus the
			learning of topics is guided by the output, and they can predict
			output (movie ratings and whatnot, as seen in the abstract).
	},
}

@unpublished{blei-2009-turbo-topics,
	title={{Visualizing Topics with Multi-Word Expressions}},
	author={Blei, D.M. and Lafferty, J.D.},
	note={{arXiv:0907.1013v1 [stat.ML]}},
	year={2009},
	abstract = { We describe a new method for visualizing topics, the
		distributions over terms that are automatically extracted from large
			text corpora using latent variable models. Our method finds
			significant n-grams related to a topic, which are then used to help
			understand and interpret the underlying distribution. Compared with
			the usual visualization, which simply lists the most probable
			topical terms, the multi-word expressions provide a better
			intuitive impression for what a topic is “about.” Our approach is
			based on a language model of arbitrary length expressions, for
			which we develop a new methodology based on nested permutation
			tests to find significant phrases. We show that this method
			outperforms the more standard use of $\chi^2$ and likelihood ratio
			tests. We illustrate the topic presentations on corpora of
			scientific abstracts and news articles.
	},
	annote = { Matt: This paper shows how to get n-grams out of a topic model
		that only returned a list of words (i.e., not Topical N-Grams)
	},
}

@incollection{blei-2009-topic-models,
	author={Blei, D.M. and Lafferty, J.D.},
	title={{Topic Models}},
	booktitle={{Text Mining: Classification, Clustering, and Applications}},
	year={2009},
	editor={Srivastava, Ashok and Sahami, Mehran},	
	publisher={Taylor and Francis},
	abstract = {},
	annote = {},
}

@misc{blei-arxiv-corpus-browser,
	author={Blei, D.M.},
	title={{50-topic-browser of latent Dirichlet allocation fit to the 2006
		arXiv}},
	note={http://topics.cs.princeton.edu/arxiv/browser50/. Accessed 10/21/2010},
	abstract = {},
	annote = {},
}

@conference{branavan-2008-lda-with-free-text-annotations,
	title={{Learning document-level semantic properties from free-text annotations}},
	author={Branavan, SRK and Chen, H. and Eisenstein, J. and Barzilay, R.},
	booktitle={Proceedings of the Annual Conference of the Association for Computational Linguistics},
	year={2008},
	organization={Citeseer},
	abstract = { This paper demonstrates a new method for leveraging
		unstructured annotations to infer semantic document properties. We
			consider the domain of product reviews, which are often annotated
			by their authors with free-text keyphrases, such as ``a real
			bargain'' or ``good value.'' We leverage these unstructured
			annotations by clustering them into semantic properties, and then
			tying the induced clusters to hidden topics in the document text.
			This allows us to predict relevant properties of unannotated
			documents. Our approach is implemented in a hierarchical Bayesian
			model with joint inference, which increases the robustness of the
			keyphrase clustering and encourages document topics to correlate
			with semantically meaningful properties. We perform several
			evaluations of our model, and find that it substantially
			outperforms alternative approaches.
	},
	annote = { Matt: I only skimmed it, but it's definitely similar to the kind
		of supervised LDA that I'm thinking of doing.  It's a paper to read
			soon.
   	},
}

@conference{brody-2010-aspect-sentiment-model-for-reviews,
	title={{An unsupervised aspect-sentiment model for online reviews}},
	author={Brody, S. and Elhadad, N.},
	booktitle={Human Language Technologies: The 2010 Annual Conference of the
		North American Chapter of the Association for Computational
			Linguistics},
	pages={804--812},
	year={2010},
	organization={Association for Computational Linguistics},
	abstract = { With the increase in popularity of online review sites comes a
		corresponding need for tools capable of extracting the information most
			important to the user from the plain text data. Due to the
			diversity in products and services being reviewed, supervised
			methods are often not practical. We present an unsupervised system
			for extracting aspects and determining sentiment in review text.
				The method is simple and flexible with regard to domain and
					language, and takes into account the influence of aspect on
					sentiment polarity, an issue largely ignored in previous
					literature. We demonstrate its effectiveness on both
					component tasks, where it achieves similar results to more
					complex semi-supervised methods that are restricted by
					their reliance on manual annotation and extensive knowledge
					sources.
	},
	annote = {},
}

@conference{chang-2009-reading-tea-leaves,
	title={{Reading tea leaves: How humans interpret topic models}},
	author={Chang, J. and Boyd-Graber, J. and Gerrish, S. and Wang, C. and
		Blei, D.M.},
	booktitle={Neural Information Processing Systems},
	year={2009},
	organization={Citeseer},
	abstract = { Probabilistic topic models are a popular tool for the
		unsupervised analysis of text,providing both a predictive model of
			future text and a latent topic representation of the corpus.
			Practitioners typically assume that the latent space is
			semantically meaningful. It is used to check models, summarize the
			corpus, and guide exploration of its contents. However, whether the
			latent space is interpretable is in need of quantitative evaluation.
			In this paper, we present new quantitative methods for measuring
			semantic meaning in inferred topics. We back these measures
			with large-scale user studies, showing that they capture aspects of
			the model that are undetected by previous measures of model quality
			based on held-out likelihood. Surprisingly, topic models which
			perform better on held-out likelihood may infer less semantically
			meaningful topics.
	},
	annote = { Matt: I haven't read this paper yet, but I've heard a lot about
		it.  They try to find a metric that correlates well with human
			evaluations of topic models, and I don't think they find a really
			good one.
	},
}

@inproceedings{ganu-2009-restaurant-ratings,
	author = {G. Ganu and N. Elhadad and A. Marian},
	booktitle = {WebDB},
	title = {Beyond the Stars: Improving Rating Predictions using Review Text
		Content.},
	year = 2009,
}

@article{griffiths-2004-finding-scientific-topics,
	title={{Finding scientific topics}},
	author={Griffiths, T. and Steyvers, M.},
	journal={Proceedings of the National Academy of Sciences},
	volume={101},
	number={Suppl 1},
	pages={5228},
	year={2004},
	publisher={National Acad Sciences},
	abstract = { A first step in identifying the content of a document is
		determining which topics that document addresses. We describe a
			generative model for documents, introduced by Blei, Ng, and Jordan
			[Blei, D. M., Ng, A. Y. \& Jordan, M. I. (2003) J. Machine Learn.
			Res. 3, 993-1022], in which each document is generated by choosing
			a distribution over topics and then choosing each word in the
			document from a topic selected according to this distribution. We
			then present a Markov chain Monte Carlo algorithm for inference in
			this model. We use this algorithm to analyze abstracts from PNAS by
			using Bayesian model selection to establish the number of topics.
			We show that the extracted topics capture meaningful structure in
			the data, consistent with the class designations provided by the
			authors of the articles, and outline further applications of this
			analysis, including identifying "hot topics" by examining temporal
			dynamics and tagging abstracts to illustrate semantic content.
	},
	annote = { Matt: This was the paper that introduced Gibbs Sampling for LDA,
		and they had a bunch of interesting analyses of the results.
	},
}

@article{griffiths-2005-integrating-topics-and-syntax,
	title={{Integrating topics and syntax}},
	author={Griffiths, T.L. and Steyvers, M. and Blei, D.M. and Tenenbaum,
		J.B.},
	journal={Advances in neural information processing systems},
	volume={17},
	pages={537--544},
	year={2005},
	publisher={Citeseer},
	abstract = { Statistical approaches to language learning typically focus on
		either short-range syntactic dependencies or long-range semantic
			dependencies between words. We present a generative model that uses
			both kinds of dependencies, and is capable of simultaneously
			finding syntactic classes and semantic topics despite having no
			knowledge of syntax or semantics beyond statistical dependency.
			This model is competitive on tasks like part-of-speech tagging and
			document classification with models that exclusively use short- and
			long-range dependencies respectively.
	},
	annote = { Matt: This is an important paper that describes what you could
		call HMM-LDA, integrating an HMM into LDA in order to distinguish
			between function words and content words.  It seems to do a decent
			job, though I would say that there are many words that I would call
			content words that end up as function words in their model.  The
			paper was, as they say, the first to integrate syntactic
			information into a topic model.
	},
}

@conference{lau-2010-best-topic-word-selection,
	title={{Best Topic Word Selection for Topic Labelling}},
	author={Lau, Jey Han and Newmann, David and Karimi, Sarvnaz and Baldwin,
		Timothy},
	booktitle={Proceedings of the 23rd International Conference on
		Computational Linguistics},
	year={2010},
	abstract = { This paper presents the novel task of best topic word
		selection, that is the selection of the topic word that is the best
			label for a given topic, as a means of enhancing the interpretation
			and visualisation of topic models. We propose a number of features
			intended to capture the best topic word, and show that, in
			combination as inputs to   a reranking model, we are able to
			consistently achieve results above the baseline of simply selecting
			the highest-ranked topic word. This is the case both when training
			in-domain over other labelled topics for that topic model, and
			cross-domain, using only labellings from independent topic models
			learned over document collections from different domains and
			genres.
	},
	annote = { Matt: This is an interesting paper, quite relevant to using
		topic models in corpus browsing.  As they note in their discussion, I
			think the restrictiveness of picking only a word from the list of
			top 10 words in a topic is a bit too limiting.  They claim that 
			their results show that it's ok 50\% of the time, but I'm not
			convinced.  All that they showed was that 50\% of the time, there
			was a word in the list that everyone agreed was the best out of the
			list.  They didn't show that there was not a better word not
			contained in the top 10 list.
	},
}

@misc{mallet,
	author={Andrew Kachites McCallum},
	title={{MALLET: A Machine Learning for Language Toolkit}},
	year={2002},
	url={http://mallet.cs.umass.edu},
	abstract = {},
	annote = {},
}

@conference{mei-2007-automatic-labeling-of-topic-models,
	title={{Automatic labeling of multinomial topic models}},
	author={Mei, Q. and Shen, X. and Zhai, C.X.},
	booktitle={Proceedings of the 13th ACM SIGKDD international conference on
		Knowledge discovery and data mining},
	pages={499},
	year={2007},
	organization={ACM},
	abstract = { Multinomial distributions over words are frequently used to
		model topics in text collections. A common, major challenge in applying
			all such topic models to any text mining problem is to label a
			multinomial topic model accurately so that a user can interpret the
			discovered topic. So far, such labels have been generated manually
			in a subjective way. In this paper, we propose probabilistic
			approaches to automatically labeling multinomial topic models in an
			objective way. We cast this labeling problem as an optimization
			problem involving minimizing Kullback-Leibler divergence between
			word distributions and maximizing mutual information between a
			label and a topic model. Experiments with user study have been done
			on two text data sets with different genres.The results show that
			the proposed labeling methods are quite effective to generate
			labels that are meaningful and useful for interpreting the
			discovered topic models. Our methods are general and can be applied
			to labeling topics learned through all kinds of topic models such
			as PLSA, LDA, and their variations.
	},
	annote = { Matt: The paper is interesting; they tried some techniques that
		I had thought about, including using the context of the documents from
			which the topics came in order to try to label the topic.  However,
		they don't look at the context of the topic in the documents; they just
			run a chunking or collocation algorithm on the text, then match
			labels to topics.  I was pretty unsatisfied with their results;
		they didn't seem very meaningful to me, because they just had three
			annotators and a few topics.},
}

@conference{mimno-2008-topic-models-with-arbitrary-features-dmr,
	title={{Topic models conditioned on arbitrary features with
		dirichlet-multinomial regression}},
	author={Mimno, D. and McCallum, A.},
	booktitle={Proceedings of the 24th Annual Conference on Uncertainty in
		Artificial Intelligence},
	organization={Citeseer},
	year={2008},
	abstract = { Although fully generative models have beensuccessfully used to
		model the contents oftext documents, they are often awkward toapply to
			combinations of text data and doc-ument metadata. In this paper we
			proposea Dirichlet-multinomial regression (DMR)topic model that
			includes a log-linear prior ondocument-topic distributions that is
			a func-tion of observed features of the document,such as author,
		publication venue, references,and dates. We show that by selecting
			ap-propriate features, DMR topic models canmeet or exceed the
			performance of severalpreviously published topic models designedfor
			specific data.
	},
	annote = { Matt: They add arbitrary conditioning features to LDA, using a
		conditional instead of a generative model.  Their results don't say
			that their technique is better than everyone else's, like
			Author-Topic and Topical N-Grams, but the model is simpler and more
			powerful.
	},
}

@conference{misra-2008-lda-to-find-semantically-incoherent-documents,
	title={{Using LDA to detect semantically incoherent documents}},
	author={Misra, H. and Capp{\'e}, O. and Yvon, F.},
	booktitle={Proceedings of the Twelfth Conference on Computational Natural
		Language Learning},
	pages={41--48},
	year={2008},
	organization={Association for Computational Linguistics},
	abstract = { Detecting the semantic coherence of a document is a
		challenging task and has several applications such as in text
			segmentation and categorization. This paper is an attempt to
			distinguish between a 'semantically coherent' true document and a
			'randomly generated' false document through topic detection in the
			framework of latent Dirichlet analysis. Based on the premise that a
			true document contains only a few topics and a false document is
			made up of many topics, it is asserted that the entropy of the
			topic distribution will be lower for a true document than that for
			a false document. This hypothesis is tested on several false
			document sets generated by various methods and is found to be
			useful for fake content detection applications.
	},
	annote = { Matt: I thought this paper would be more interesting.  All they
		do is use the entropy of the topic distribution in an attempt to find
			documents that are forged by spammers.  It is important because it
				tipped me to the idea of using entropy in different ways, but I
				wasn't incredibly impressed the paper overall.
	},
}

@conference{newman-2006-statistical-entity-topic-models,
	title={{Statistical entity-topic models}},
	author={Newman, D. and Chemudugunta, C. and Smyth, P.},
	booktitle={Proceedings of the 12th ACM SIGKDD international conference on
		Knowledge discovery and data mining},
	pages={686},
	year={2006},
	organization={ACM},
	abstract = { The primary purpose of news articles is to convey information
		about who, what, when and where. But learning and summarizing these
			relationships for collections of thousands to millions of articles
			is difficult. While statistical topic models have been highly
			successful at topically summarizing huge collections of text
			documents, they do not explicitly address the textual interactions
			between who/where, i.e. named entities (persons, organizations,
					locations) and what, i.e. the topics. We present new
			graphical models that directly learn the relationship between
			topics discussed in news articles and entities mentioned in each
			article. We show how these entity-topic models, through a better
			understanding of the entity-topic relationships, are better at
			making predictions about entities.
	},
	annote = { Matt: A pretty interesting paper.  They include named entities
		into a topic model, and use the results in various tasks.  It's most
			applicable to what I'm thinking of doing just because of their use
			of named entities with topic models, and its application to corpus
			browsing (they didn't explicitly mention it, but it's pretty
					obvious).
	},
}

@conference{newman-2010-automatic-evaluation-of-topic-coherence,
	title={{Automatic evaluation of topic coherence}},
	author={Newman, D. and Lau, J.H. and Grieser, K. and Baldwin, T.},
	booktitle={NAACL HLT},
	year={2010},

	abstract = { This paper introduces the novel task of topic coherence
		evaluation, whereby a set of words, as generated by a topic model, is
			rated for coherence or interpretability. We apply a range of topic
			scoring models to the evaluation task, drawing on WordNet,
		Wikipedia and the Google search engine, and existing research on
			lexical similarity/relatedness. In comparison with human scores for
			a set of learned topics over two distinct datasets, we show a
			simple co-occurrence measure based on point-wise mutual information
			over Wikipedia data is able to achieve results for the task at or
			nearing the level of inter-annotator correlation, and that other
			Wikipedia-based lexical relatedness methods also achieve strong
			results. Google produces strong, if less consistent, results, while
			our results over WordNet are patchy at best.
	},
	annote = { Matt: This paper introduces using mutual information between
		words as calculated from Wikipedia in order to evaluate topic coherence.
			They say that the performance of that metric is as good as human
			evaluation.
	},
}

@article{newman-2010-visualizing-with-topic-maps,
	title={{Visualizing search results and document collections using topic
		maps}},
	author={D. Newman et al},
	journal={Web Semantics: Science, Services and Agents on the World Wide
		Web},
	year={2010},
	publisher={Elsevier},
	abstract = { This paper explores visualizations of document collections,
		which we call topic maps. Our topic maps are based on a topic model of
			the document collection, where the topic model is used to determine
			the semantic content of each document. Using two collections of
			search results, we show how topic maps reveal the semantic
			structure of a collection and visually communicate the diversity of
			content in the collection. We describe techniques for assessing the
			validity and accuracy of topic maps, and discuss the challenge of
			producing useful two-dimensional maps of documents.
	},
	annote = {},
}

@conference{rosen-zvi-2004-author-topic-model,
	title={{The author-topic model for authors and documents}},
	author={Rosen-Zvi, M. and Griffiths, T. and Steyvers, M. and Smyth, P.},
	booktitle={Proceedings of the 20th conference on Uncertainty in artificial
		intelligence},
	pages={494},
	year={2004},
	organization={AUAI Press},
	abstract = { We introduce the author-topic model, a generative model for
		documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, and
				Jordan, 2003) to include authorship information. Each author is
			associated with a multinomial distribution over topics and each
			topic is associated with a multinomial distribution over words. A
			document with multiple authors is modeled as a distribution over
			topics that is a mixture of the distributions associated with the
			authors. We apply the model to a collection of 1,700 NIPS
			conference papers and 160,000 CiteSeer abstracts. Exact inference
			is intractable for these datasets and we use Gibbs sampling to
			estimate the topic and author distributions. We compare the
			performance with two other generative models for documents, which
			are special cases of the author-topic model: LDA (a topic model)
			and a simple author model in which each author is associated with a
			distribution over words rather than a distribution over topics. We
			show topics recovered by the author-topic model, and demonstrate
			applications to computing similarity between authors and entropy of
			author output.
	},
	annote = {},
}

@article{si-2009-tag-lda-for-tag-recommendation,
	title={{Tag-LDA for Scalable Real-time Tag Recommendation}},
	author={Si, X. and Sun, M.},
	journal={Journal of Information \& Computational Science},
	volume={6},
	number={1},
	pages={23--31},
	year={2009},
	abstract = { In this paper, we propose a scalable and real-time method for
		tag recommendation. We model document, words and tags using tag-LDA
			model, which extends Latent Dirichlet Allocation model by adding
			the tag variable. With tag-LDA model, we can make real-time
			inference about the likelihood of assigning a tag to a new
			document, and use the likelihood to generate recommended tags. To
			handle large scale data set from the web, we implement a
			distributed training program, which can train the tag-LDA model in
			parallel with multiple machines. We use a real world blog data set
			containing 386,012 documents to evaluate our method. The
			distributed training program can handle the data set efficiently.
			The tags recommended by our method are 32\% better than
			search-based collaborative filtering. We also analyze some examples
			to show the ability and weakness of our method.
	},
	annote = { Matt: I have yet to really read this paper, but they add a set
		of tags that are generated by the topics---they learn topic-tag
			distributions along with topic-word distributions, and use those
			distributions to predict tags on untagged instances.
	},
}

@article{steyvers-2010-combining-background-knowledge-with-topics,
	title={{Combining background knowledge and learned topics}},
	author={Steyvers, M. and Smyth, P. and Chemuduganta, C.},
	journal={Topics in Cognitive Science},
	publisher={John Wiley \& Sons},
	year={2010},
	abstract = {     Statistical topic models provide a general data-driven
		framework for automated discovery of high-level knowledge from large
			collections of text documents. Although topic models can
			potentially discover a broad range of themes in a data set, the
			interpretability of the learned topics is not always ideal.
			Human-defined concepts, however, tend to be semantically richer due
			to careful selection of words that define the concepts, but they
			may not span the themes in a data set exhaustively. In this study,
		we review a new probabilistic framework for combining a hierarchy of
			human-defined semantic concepts with a statistical topic model to
			seek the best of both worlds. Results indicate that this
			combination leads to systematic improvements in generalization
			performance as well as enabling new techniques for inferring and
			visualizing the content of a document.
	},
	annote = { Matt: The paper was somewhat interesting, though not as good as
		I was expecting given the title.  They deal with a concept-topic model
			that uses human-defined "concepts," or groups of related words that
			have an associated label.  The don't ever use the label except for
			visualizing the output, however.  The results were alright but not
			very impressive.  The discussion section has references to other
			papers that might be relevant.
	},
}

@conference{wang-2006-topics-over-time,
	title={{Topics over time: a non-Markov continuous-time model of topical
		trends}},
	author={Wang, X. and McCallum, A.},
	booktitle={Proceedings of the 12th ACM SIGKDD international conference on
		Knowledge discovery and data mining},
	pages={424--433},
	year={2006},
	organization={ACM},
	abstract = { This paper presents an LDA-style topic model that captures not
		only the low-dimensional structure of data, but also how the structure
			changes over time. Unlike other recent work that relies on Markov
			assumptions or discretization of time, here each topic is
			associated with a continuous distribution over timestamps, and for
			each generated document, the mixture distribution over topics is
			influenced by both word co-occurrences and the document's
			timestamp. Thus, the meaning of a particular topic can be relied
			upon as constant, but the topics' occurrence and correlations
			change significantly over time. We present results on nine months
			of personal email, 17 years of NIPS research papers and over 200
			years of presidential state-of-the-union addresses, showing
			improved topics, better timestamp prediction, and interpretable
			trends.},
	annote = {},
}

@conference{wang-2007-topical-ngrams,
	title={{Topical n-grams: Phrase and topic discovery, with an application to
		information retrieval}},
	author={Wang, X. and McCallum, A. and Wei, X.},
	booktitle={Proceedings of the 7th IEEE international conference on data
		mining},
	pages={697--702},
	year={2007},
	organization={Citeseer},
	abstract = { Most topic models, such as latent Dirichlet allocation,rely on
		the bag-of-words assumption. However, word orderand phrases are often
			critical to capturing the meaning oftext in many text mining tasks.
			This paper presents topicaln-grams, a topic model that discovers
			topics as well as top-ical phrases. The probabilistic model
			generates words intheir textual order by, for each word, first
			sampling a topic,then sampling its status as a unigram or bigram,
		and thensampling the word from a topic-specific unigram or
			bigramdistribution. Thus our model can model “white house” asa
			special meaning phrase in the ‘politics’ topic, but notin the ‘real
			estate’ topic. Successive bigrams form longerphrases. We present
			experimental results showing mean-ingful phrases and more
			interpretable topics from the NIPSdata and improved information
			retrieval performance on aTREC collection.
	},
	annote = { Matt: The paper that introduced Topical N-grams, an important
		extension of LDA that captures phrases as well as individual words for
			the topics.  TNG is implemented in Mallet.
	},
}

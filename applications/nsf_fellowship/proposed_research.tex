\documentclass[onecolumn, 12pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{natbib}

\setlength{\textheight}{9in}
\setlength{\bibsep}{0in}

\begin{document}

\pagestyle{empty}

\begin{center}
  \textbf{Searching Document Collections by Topic}
\end{center}
\vspace{-4mm}

\textbf{Keywords.} \textit{Topic models, natural language processing,
information extraction}

\textbf{Introduction.}  In our modern world, large document collections are
becoming more prevalent and more important.  Companies want to look at online
reviews of their products and intelligence analysts have thousands of documents
to sift through looking for suspicious activity.  The recent controversies
involving the public release of hundreds of thousands of documents by WikiLeaks
about the wars in Afghanistan and Iraq highlight the enormity of this problem.
How does a person even begin to look through these massive corpora?

Typically, the interface between a person and a document collection has been a
search engine, using a Google-like tool to find documents that contain specific
information.  While such tools can be very useful for some tasks, they cannot
give broad pictures about the corpus as a whole, and they are limited in the
kinds of queries they can answer.  A person has to have specific information in
mind in order to form a query; current search tools are not very helpful when a
person has no idea what to even look for.

Recent methods have been proposed that attempt to address part of this issue.
Topic models, including latent Dirichlet
allocation~\cite{blei-2003-latent-dirichlet-allocation}, gather information
about topics used in a corpus.  These topics are modeled as collections of
words that are used together across documents.  Topic models could feasibly be
used to give a broad overview of a corpus and even answer queries about topics
in the documents, instead of just queries about the documents.

As far as I am aware, however, topic models have not as yet been used for this
purpose.  They have successfully been used as input into document
classification and many other tasks
(e.g.,~\cite{blei-2008-supervised-topic-models}), but attempts to aid humans in
corpus analysis through topic models are very preliminary, mostly giving the
user better access to the documents in the corpus, not to the topics
themselves~\cite{newman-2010-visualizing-with-topic-maps}.  It is generally
assumed that topic models do a good job at capturing thematic elements in large
bodies of text, but while this has been anecdotally asserted on many
occasions~\cite{griffiths-2004-finding-scientific-topics}, to my knowledge it
has never been rigorously established.  Thus topic models have not been widely
used in the qualitative analysis of large corpora.

\textbf{Research Objective.}  It is my intent to quantitatively show that topic
models can effectively capture qualitative or thematic elements in a collection
of documents as judged by humans, and that automated methods can select topics
from a topic model that match human thematic judgments in a variety of
situations.  As an example, given the set of campaign speeches for all
candidates in the 2008 presidential primaries and general election, this
research will show conclusively that a topic model can accurately answer the
question, ``What were Barack Obama's consistent campaign themes?''

\textbf{Methods.}  Establishing that topic models can answer topic-based
queries the same way a human would requires two parts.  First one must show
that a topic found by a topic model can be accurately compared to a theme or
topic elicited from a human.  Second, one must show that an automated system
can correctly answer queries about topics in a corpus by returning the topics
from a topic model that match human answers to the same queries.

In order to establish these two parts, I will need a large collection of
documents annotated with specific topical information.  The necessary
annotations include human judgments on what themes or topics are present in the
documents (giving a phrasal description of each topic), where they occur
(including documents that demonstrate the topic well), and how they occur
(giving example words or phrases from the documents).  To my knowledge there is
no such annotated data, so I will create it.  

I will use two sets of data to provide more conclusive results.  The first is a
collection of free-form student comments on classes and professors given to
Brigham Young University at the end of each semester.  The second is a publicly
available set of restaurant reviews from CitySearch New
York~\cite{ganu-2009-restaurant-ratings}.  Neither of these sets of data are
annotated with sufficient topical information, though the restaurant reviews
are already partially annotated.

As there are many thousands of documents in each set of data, annotating the
entire corpus would be impossible.  Thus I will only annotate a section of each
collection, corresponding to all of the comments about one particular class in
the student ratings, or one restaurant in the reviews.  This will aid in
validation, as I will have complete topical information about one of the
classes or restaurants and will thus be able to accurately test the answers to
queries.  I will hire between 20 and 40 college students to each independently
annotate the same subset of one of the collections of documents, so that I can
have confidence in the data and have some idea of the consistency of human
thematic judgments.

\textbf{Validation.}  Once I have annotated data, I will be able to test the
validity of my hypothesis.  Determining whether a particular topic mathces a
human-defined topic should be straightforward with the annotations I will
gather.  I can test the words found by the topic model against the description
given by the humans, and I can see if the example documents given by the
annotators agree with the documents that have high proportions of the topic.

With methods for matching topics from a topic model with human-given topics, I
can complete the second part of my objective.  With topics obtained from humans
for a particular class, I can learn a topic model over the entire collection of
student ratings and see if automatic methods to rank the topics return the
correct topics.  One such method is to rank the topics by their entropy over
the documents for that class, which should rank highest the topics that were
used consistently about that class, matching the human-given topics and
answering the question, ``What were the main themes in comments of Chemistry
101?''  Other possible queries and methods to answer them are items that will
be explored in the course of this research.

\textbf{Broader Impacts.}  This work will quantitatively establish the
usefulness of topic models in extracting thematic elements from a collection of
documents.  It will also validate the automatic ranking of topics in response
to a few classes of specific user queries.  Such work will give confidence to
anyone using a topic model to analyze a corpus, including intelligence analysts
looking at classified documents, university administrators looking at student
ratings, and companies reading online reviews about their products.  This work
also establishes a method and a set of data for topic models to be compared to
each other in order to determine which models best capture human thematic
judgments.

\footnotesize
\bibliographystyle{plain}
\renewcommand\bibsection{\noindent \small\textbf{References}\vspace{-2mm}\footnotesize}
\bibliography{../../../bib/lda/bib}

\end{document}

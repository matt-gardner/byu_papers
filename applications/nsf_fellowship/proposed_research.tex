\documentclass[onecolumn, 12pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{natbib}

\setlength{\textheight}{9in}

\begin{document}

\pagestyle{empty}

\begin{center}
  Matthew Gardner\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
  Proposed Research Essay
  \\
  \textbf{Searching Document Collections by Topic}
\end{center}
\vspace{-4mm}

\textbf{Keywords.} \textit{Topic models, natural language processing,
information extraction}

\textbf{Introduction.}  In our modern world, large document collections are
becoming more prevalent and more important.  Companies want to look at online
reviews of their products, students beginning research in a new area have piles
of documents to read, and intelligence analysts have thousands of documents to
sift through looking for anything noteworthy.  The recent controversies
involving the release of hundreds of thousands of documents by WikiLeaks about
the wars in Afghanistan and Iraq highlight the enormity of this problem.  How
does a person even begin to look through these massive corpora?

Typically, the interface between a person and a document collection has been
search, using a Google-like tool to find documents that contain specific
information.  While such tools can be very useful for some tasks, they cannot
give broad pictures about the corpus as a whole, and they are limited in the
kinds of queries they can answer.  A person has to have something to look for
in order to form a query; current search tools are not very helpful when a
person has no idea what to even look for.

Recent methods have been proposed that attempt to address part of this issue.
Topic models, including latent Dirichlet allocation and its
variants~\cite{blei-2003-latent-dirichlet-allocation}, seek to gather
information about topics used throughout the corpus.  These topics are modeled
as collections of words that are used together across documents.  Topic models,
then, could feasibly be used to give a broad overview of a corpus and even
answer queries about topics in the documents, instead of just queries about the
documents themselves.

As far as I am aware, however, topic models have not as yet been used for this
purpose.  They have successfully been used as input into document
classification tasks, for sentiment extraction, and as a surrogate for parts of
speech in machine translation (e.g.,~\cite{blei-2008-supervised-topic-models}).
But attempts to improve human corpus analysis through topic models are very
preliminary and have mostly been used to give the user better access to the
documents, not to the topics
themselves~\cite{newman-2010-visualizing-with-topic-maps}.  It is generally
assumed that topic models do a good job at capturing thematic elements in large
bodies of text, but while this has been anecdotally asserted on many
occasions~\cite{griffiths-2004-finding-scientific-topics}, to my knowledge it
has never been rigorously established.  Thus topic models have not been widely
used in the qualitative analysis of large corpora.

\textbf{Research Objective.}  It is my intent to quantitatively show that
topic models can effectively capture qualitative or thematic elements in a
collection of documents as judged by humans, and that automated methods can
select topics from a topic model that match human thematic judgments in a
variety of situations.

\textbf{Methods.}  In order to establish this, I will need a large collection
of documents annotated with specific topical information.  The necessary
annotations include human judgments on what themes or topics are present in
the documents, where they occur, and how they occur.  To my knowledge there is
no such set of data, so I will create one.  

To get more conclusive results, I will collect and annotate two separate
collections of documents.  The first comes from student ratings of courses and
professors at Brigham Young University.  After answering many questions which
require selecting one of several options, students are given the opportunity to
write in free text whatever comments they have about the class.  I will use
those free-form comments as one source of text in which to find thematic
elements.

The second source of data I will use is a publicly available set of restaurant
reviews from CitySearch New York~\cite{ganu-2009-restaurant-ratings}.  This
dataset has the advantage of already being partially labeled, so providing the
rest of the annotations necessary for my purposes should be less expensive.

I will annotate only a subset of the data.  As there are hundreds of thousands
of comments available from the student ratings data that we will use, and over
50,000 restaurant reviews, reading and labeling every document with thematic
elements is an impossible task.  Instead, I will select a subset of the data
that is large enough to exhibit themes but small enough that an average college
student can annotate the data in approximately half an hour.  I imagine that
somewhere between 50 to 100 comments (where each comment is about a paragraph
long) is an appropriate size.  I will judge the amount of time required by
annotating the data myself, then hire between 10 and 20 college students to
also annotate the data, so that I can be sure that the annotations accurately
reflect average human judgments.  

An important aspect of the subset of the data that to be annotated will be that
it will consist of a set of documents that all have the same value for some
attribute, such as ratings for the same teacher, or reviews about the same
restaurant.  This will facilitate both the finding of consistent thematic
elements and more extensive kinds of validation, which will be explained later.

The kinds of information I will elicit from the annotators will include the
top overall themes (ordered by prevalence) in the whole collection of documents
they are presented, as well as where those themes show up in the documents.  It
would likely be helpful for the annotator to select for each theme a single
document that best exhibits that theme, as well as list words that are commonly
used in the theme.  All of this will help me to judge whether a particular
topic from the topic model actually matches the theme given by the annotators.

\textbf{Validation.}  There are two parts to validating the use of topic models
in the qualitative analysis of text.  The first is showing that topic models
actually capture the thematic elements in a set of documents, as judged by
humans.  The second is showing that automatic methods for ranking the topics in
a topic model accurately rank topics according to the ranking given by humans.
Each part requires separate validation.

To demonstrate that topic models capture thematic elements, a mechanism is
necessary to compare a topic model's topic to the themes recorded by humans.
Devising this similarity metric will require some research, but as a starting
point I will compare the words given by the annotators to the words in the
topic, as well as the top documents given by the annotators to the documents
with the highest proportion of each topic.  I will also try to use some kind
of mutual information between the words in the topic and the title or
description of each theme as given by the annotators.

In order to show that automatic rankings can faithfully reproduce human
rankings, I will use standard metrics from information retrieval (IR), such as
mean average precision, to compare the two rankings.  This validation depends
on the first, as I must be able to determine if the topics ranked by the
computer match the themes ranked by humans.  This can be problematic if there
is no topic in the topic model that successfully captures a human theme, or if
there are two topics that each capture the theme equally well.  I may have to
modify the standard IR metrics to be better suited to my task.  While I will
only be annotating a subset of each data collection, I will be able to use the
entire corpus to validate my methods.  An important question to be able to
answer correctly is, ``What topics occur in reviews about McDonald's?''
Learning a topic model on the entire data and ranking the topics in answer to
such specific questions will be a very useful validation.

\textbf{Broader Impacts.}  This work will quantitatively establish the
usefulness of topic models in extracting thematic elements from a collection of
documents.  It will also validate the automatic ranking of topics in response
to a few classes of specific user queries.  Such work will give confidence to
anyone using a topic model to analyze a corpus, including intelligence analysts
looking at classified documents, university administrators looking at student
ratings, and companies reading online reviews about their products.  This work
also establishes a method and a set of data for topic models to be compared to
each other, for one model may better capture that human-judged thematic
elements than another.

\footnotesize
\bibliographystyle{plain}
\renewcommand\bibsection{\subsubsection*{References}}
\bibliography{../../../bib/lda/bib}

\end{document}

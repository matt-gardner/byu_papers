\documentclass[onecolumn, 12pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{natbib}

\setlength{\textheight}{9in}
\setlength{\bibsep}{0in}

\begin{document}

\pagestyle{empty}

\begin{center}
  \textbf{Searching Document Collections by Topic}
\end{center}
\vspace{-4mm}

\textbf{Keywords.} \textit{Topic models, natural language processing,
information extraction, search}

\textbf{Introduction.}  In our modern world, large document collections are
becoming more prevalent and more important.  Companies want to look at online
reviews of their products and intelligence analysts have thousands of documents
to sift through looking for suspicious activity.  The recent controversies
involving the public release of hundreds of thousands of documents by WikiLeaks
about the wars in Afghanistan and Iraq highlight the enormity of this problem.
How does a person even begin to look through these massive corpora?

Typically, the interface between a person and a document collection has been a
search engine, using a Google-like tool to find documents that contain specific
information.  While such tools can be very useful for some tasks, they cannot
give broad pictures about the corpus as a whole, and they are limited in the
kinds of queries they can answer.  A person has to have specific information in
mind in order to form a query; current search tools are not very helpful when a
person has no idea what to even look for.

Recent methods have been proposed that attempt to address part of this issue.
Topic models, including latent Dirichlet
allocation~\cite{blei-2003-latent-dirichlet-allocation}, gather information
about topics used in a corpus.  These topics are modeled as collections of
words that are used together across documents.  Topic models could feasibly be
used to give a broad overview of a corpus and even answer queries about topics
in the documents, instead of just queries about the documents.

As far as I am aware, however, topic models have not as yet been used for this
purpose.  They have successfully been used as input into document
classification and many other tasks
(e.g.,~\cite{blei-2008-supervised-topic-models}), but attempts to aid humans in
corpus analysis through topic models are very preliminary, mostly giving the
user better access to the documents in the corpus, not to the topics
themselves~\cite{newman-2010-visualizing-with-topic-maps}.  It is generally
assumed that topic models do a good job at capturing thematic elements in large
bodies of text, but while this has been anecdotally asserted on many
occasions~\cite{griffiths-2004-finding-scientific-topics}, to my knowledge it
has never been rigorously established.  Thus topic models have not been widely
used in the qualitative analysis of large corpora.

\textbf{Research Objective.}  It is my intent to quantitatively show that topic
models can effectively capture qualitative or thematic elements in a collection
of documents as judged by humans, and that automated methods can select topics
from a topic model that match human thematic judgments in a variety of
situations.  As an example, given the set of campaign speeches for all
candidates in the 2008 presidential primaries and general election, this
research will show conclusively that a topic model can accurately answer the
question, ``What were Barack Obama's consistent campaign themes?''

\textbf{Methods.}  Establishing that topic models can answer topic-based
queries the same way a human would involves two parts.  First one must show
that a topic found by a topic model can be accurately compared to a theme or
topic elicited from a human.  Second, one must show that an automated system
can correctly answer queries about topics in a corpus by returning the topics
from a topic model that match human answers to the same queries.

In order to establish these two parts, I will need a large collection of
documents annotated with specific topical information.  The necessary
annotations include human judgments on what themes or topics are present in the
documents (giving a phrasal description of each topic), where they occur
(including documents that demonstrate the topic well), and how they occur
(giving example words or phrases from the documents).  To my knowledge there is
no such annotated data, so I will create it.  

I will use two sets of data to provide more conclusive results.  The first is a
collection of free-form student comments on classes and professors given to
Brigham Young University at the end of each semester.  The second is a publicly
available set of restaurant reviews from CitySearch New
York~\cite{ganu-2009-restaurant-ratings}.  Neither of these sets of data are
annotated with sufficient topical information, though the restaurant reviews
are already partially annotated.

As there are many thousands of documents in each set of data, annotating the
entire corpus would be impossible.  Thus I will only annotate a section of each
collection, corresponding to all of the comments about one particular class in
the student ratings, or one restaurant in the reviews.  This will aid in
validation, as I will have complete topical information about one of the
classes or restaurants and will thus be able to accurately test the answers to
queries.  I will hire between 20 and 40 college students to each independently
annotate the same subset of one of the collections of documents, so that I can
have confidence in the data and have some idea of the consistency of human
thematic judgments.

\textbf{Validation.}  With annotated data, I can test the validity of my
hypothesis.  Determining whether a particular topic matches a human-defined
topic is straightforward with the annotations I will gather.  I can test the
words found by the topic model against the description given by the humans, and
I can see if the example documents given by the annotators agree with the
documents that have high proportions of the topic according to the model.

Once I have methods for matching topics from a topic model with human-given
topics, I can complete the second part of my objective.  I will learn a topic
model over each collection of documents and see if automatic methods to rank
the topics return topics that match those that were given by humans.  One such
method is to rank the topics by their entropy over the documents for the
annotated class or restaurant.  This ranking should match the human-judged
themes and answer the question, ``What were the main themes in comments about
this class?'' Similar methods can give a broad overview of the topics in the
entire corpus, and other possible queries and automatic ranking methods to
answer them are items that will be explored in the course of this research,
including finding topics that are common or unique to two separate classes.

\textbf{Broader Impacts.}  This work will quantitatively establish the
usefulness of topic models in extracting thematic elements from a collection of
documents.  It will also validate the automatic ranking of topics in response
to a few types of specific user queries.  Such work will give confidence to
anyone using a topic model to analyze a corpus, including intelligence analysts
looking at classified documents, university administrators looking at student
ratings, and companies reading online reviews about their products.  This work
also establishes a method and a set of data for topic models to be compared to
each other in order to determine which models best capture human thematic
judgments.

\footnotesize
\bibliographystyle{plain}
\renewcommand\bibsection{\noindent \small\textbf{References}\vspace{-2mm}\footnotesize}
\bibliography{bib}

\end{document}

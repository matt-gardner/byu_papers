\documentclass[onecolumn, 12pt]{article}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{natbib}

\setlength{\bibsep}{0in}

\begin{document}

\pagestyle{empty}

\begin{center}
  \textbf{Personal Statement}\\
  Matthew Gardner
\end{center}

It was spending time as a Spanish-English interpreter that first piqued my
interest in natural language processing.  What if a computer could interpret
accurately and in real time between two languages?  What problems would have to
be solved in order to build such a system?  These questions fascinated me, and
I set out to spend my undergraduate career learning as much as I could about
their answers.

In my second year of college I started as a research assistant in the Applied
Machine Learning lab at Brigham Young University (BYU).  The project that was
given to me to work on was particle swarm optimization (PSO).  While my work in
PSO did not relate directly to my interest in machine interpretation, it taught
me what it is like to do real research and prepared me for my future work as a
PhD student.

All told I spent two and a half years working on various aspects of PSO.  At
the beginning I was just running experiments with code other people wrote, a
simple bookkeeper.  I progressed to helping analyze and interpret the results,
then to writing a little code of my own.  After about a year in the lab, I got
my first publication; I was a co-author on a paper about parallelization and
scalability in PSO~\cite{mcnabb-2009-large-particle-swarms}.  

Motivated by the success of our parallel PSO paper, we spent more effort on how
to better parallelize the algorithm.  My advisor wondered if PSO could be
speculatively decomposed, allowing extra processors to do two or more
iterations of PSO at the same time.  I thought that was an interesting idea and
I made it my honors thesis, moving the idea to an algorithm and then to an
implementation.  After testing my new method and discovering that it
outperformed standard techniques, I turned my work into a paper and
successfully defended my honors thesis.  Confident that my work was amazing, I
submitted my thesis to one of the top conferences for PSO research, fully
expecting a glowing reception.

I had my first major setback when that paper was rejected outright.  At this
point I had spent two years working on PSO, and I felt like giving up and
moving on.  My advisor, however, insisted that I persevere, telling me that the
experience would be good practice for a PhD.  So I revised my paper and
submitted it to another top conference, where it was
accepted~\cite{gardner-2010-speculative-evaluation-in-pso}.

In doing the work for my honors thesis I had many ideas for how to improve the
method I pioneered.  Once I recovered from the shock of my first rejection, I
spent some time exploring new methods that gave significantly greater
performance than my original implementation.  I expanded my honors thesis with
these new ideas and submitted my work to the journal \emph{Swarm Intelligence},
where it is currently under
review~\cite{gardner-2010-speculative-approach-to-parallelization-pso}.

My work in particle swarm optimization was not directly applicable to this
program to which I am currently applying, though a general knowledge of
optimization techniques is certainly useful in many fields of computer science.
However, I feel that the experience I had working on that problem has done a
great deal to prepare me to succeed in your program.  I have experience
conducting real research, I know how to bring ideas to fruition, and I have
successfully taken my own innovations and had them recognized by the wider
research community.  I am confident that, while there will inevitably be
setbacks in any research endeavor, I can persevere and succeed.

I have had many other experiences that also contribute to my preparation for
your program.  I have gained a broad background in statistical machine learning
techniques, having been the TA for the Bayesian Inference class and taken
several advanced classes on the subject.  I have also taken all of the classes
that BYU offers in natural language processing (NLP).  For my final project in
the graduate-level NLP course, I implemented a Gibbs sampler for the latent
Dirichlet allocation model and ran it on a novel dataset of religious
discourses.  That work was of particular interest to BYU, and I presented it at
the BYU Studies Symposium, an international conference held at the
university~\cite{gardner-2010-general-conference-topics}.

At the end of my undergraduate education I applied for and received a
scholarship from the North American chapter of the Association for
Computational Linguistics (NAACL) to attend a summer school at Johns Hopkins
University on human language technologies.  My experience at JHU over the
summer was further preparation for your program, as we covered a broad range of
topics dealing with machine learning and natural language processing.  I also
attended the NAACL conference that summer and saw firsthand some of the
research going on at your school.

As a masters student at BYU I have continued my work with topic modeling,
exploring how topic models can be used to improve the browsing experience of
large corpora.  I recently returned from the NIPS conference, where I presented
a tool that I and others at BYU have built that incorporates a lot of previous
work in visualizing topic models and some of our own
ideas~\cite{gardner-2010-topic-browser}.

I have significant experience conducting and presenting research, and I have
sufficient background in machine learning and natural language processing to be
able to quickly contribute to ongoing research projects.  I would love to spend
my time as a PhD student working on machine interpretation or some subproblem
of it.  Speech recognition, speech synthesis, machine translation, parsing and
co-reference resolution are all interesting problems to me that I would enjoy
working on.  As statistical modeling is the foundation of most state-of-the-art
solutions to these problems, I also would enjoy doing research in that area.
[Add more personalized stuff to each university here]

\vspace{2mm}
\footnotesize
\bibliographystyle{plain}
\renewcommand\bibsection{\noindent \small\textbf{Publications}\vspace{-2mm}\footnotesize}
\bibliography{bib}

\end{document}

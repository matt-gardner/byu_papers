\documentclass[letterpaper]{sig-alternate}

% GECCO Submission Number: 
% GECCO Camera-Ready Number: 

\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{subfigure}
\usepackage{listings}
\usepackage{amsmath,amssymb}
\usepackage{array}
\usepackage{algorithm}

\lstloadlanguages{Python}
\floatname{algorithm}{Function}
%\lstset{numbers=left, numberstyle=\tiny}

\newlength{\figwidth}
\newlength{\graphwidth}
\newlength{\figheight}
\newlength{\figsmallheight}
\setlength{\figwidth}{0.46\textwidth}
\setlength{\graphwidth}{0.46\textwidth}
\setlength{\figheight}{0.26\textheight}
\setlength{\figsmallheight}{0.8\figheight}

% AML bibliography references
\newcommand{\bib}[1]{../../papers/#1}


\providecommand{\Norm}[2][2]{\ensuremath{\|#2\|_{#1}}}
\providecommand{\Est}[1]{\ensuremath{\widehat{#1}}}
\renewcommand{\Vec}[1]{\mathbf{#1}}
\providecommand{\Dims}{\ensuremath{D}}
\providecommand{\PosBest}{\ensuremath{{\Pos^\star}}}
\providecommand{\PosElement}{\ensuremath{x}}
\providecommand{\Pos}{\ensuremath{\Vec{\PosElement}}}
\providecommand{\PosMatrix}{\ensuremath{\Vec{X}}}
\providecommand{\Func}{\ensuremath{\mathcal{F}}}
\providecommand{\Val}{\ensuremath{y}}
\providecommand{\ValVector}{\ensuremath{\Vec{y}}}
\providecommand{\ValBest}{\ensuremath{{\Val^\star}}}
\providecommand{\Optfunc}{\ensuremath{\Omega}}
\providecommand{\Samplefunc}{\ensuremath{\Delta}}
\providecommand{\Density}{\ensuremath{\mathrm{\pi}}}
\providecommand{\Prob}{\ensuremath{\mathrm{p}}}
\providecommand{\Scale}{\ensuremath{\alpha}}
\providecommand{\Exponent}{\ensuremath{\beta}}
\providecommand{\Normal}{\ensuremath{\mathcal{N}}}
\providecommand{\Particle}{\ensuremath{\Vec{s}}}
\providecommand{\ParticleMatrix}{\ensuremath{\Vec{S}}}
\providecommand{\AdditionalState}{\ensuremath{\Vec{\phi}}}


%% PSO Stuff
\DeclareMathOperator{\URand}{U}
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\DeclareMathOperator*{\arginf}{arg\;inf}
\DeclareMathOperator*{\argsup}{arg\;sup}
\DeclareMathOperator{\Diversity}{diversity}
\providecommand{\Vmax}{\ensuremath{V_\text{max}}}
\providecommand{\ppos}{\ensuremath{\Vec{x}}}
\providecommand{\posmean}{\ensuremath{\bar{\ppos}}}
\providecommand{\pvel}{\ensuremath{\Vec{v}}}
\providecommand{\pacc}{\ensuremath{\ddot{\ppos}}}
\providecommand{\gbest}{\ensuremath{\Vec{g}}}
\providecommand{\greallybest}{\ensuremath{\Vec{g}^\star}}
\providecommand{\pbest}{\ensuremath{\Vec{p}}}
\providecommand{\bestmean}{\ensuremath{\bar{\pbest}}}
\providecommand{\momentum}{\ensuremath{\omega}}
\providecommand{\constriction}{\ensuremath{\chi}}
\providecommand{\coeff}{\ensuremath{\phi}}
\providecommand{\sttrue}{\ensuremath{\Vec{\theta}}}
\providecommand{\obs}{\ensuremath{\Vec{\xi}}}
\providecommand{\transition}{\ensuremath{\mathcal{F}}}
\providecommand{\observation}{\ensuremath{\mathcal{H}}}

%% Other notation
\providecommand{\FeasibleDiagonal}{\ensuremath{L}}
\providecommand{\Radius}{\ensuremath{r}}
\providecommand{\Adapt}{\ensuremath{\gamma}}
\providecommand{\BounceCount}{\ensuremath{b}}
\providecommand{\Swarm}{\ensuremath{S}}
\providecommand{\ArpsoLow}{\ensuremath{\eta^-}}
\providecommand{\ArpsoHigh}{\ensuremath{\eta^+}}
\providecommand{\Direction}{\ensuremath{s}}

% MapReduce notation:
\providecommand{\List}[1]{\ensuremath{\mathsf{list}(#1)}}
\providecommand{\Pair}[2]{\ensuremath{(#1, #2)}}
\providecommand{\NaturalSet}{\ensuremath{\mathbb{N}}}
\providecommand{\IntegerSet}{\ensuremath{\mathbb{Z}}}
\providecommand{\StringsSet}{\ensuremath{\mathsf{Strings}}}
\providecommand{\StateStringsSet}{\ensuremath{\mathsf{StateStrings}}}
\providecommand{\InputKeySet}{\ensuremath{K_1}}
\providecommand{\OutputKeySet}{\ensuremath{K_2}}
\providecommand{\InputValSet}{\ensuremath{V_1}}
\providecommand{\OutputValSet}{\ensuremath{V_2}}

% Make figures easier to place on the page with the text
\renewcommand{\dblfloatpagefraction}{0.70}
\renewcommand{\dbltopfraction}{0.8}
\renewcommand{\textfraction}{0.1}


%\widowpenalty=10000
%\clubpenalty=10000
\begin{document}

\hyphenation{}

\conferenceinfo{GECCO'09,} {July 8--12, 2009, Montr\'eal, Canada.}
\CopyrightYear{2009}
%\crdata{1-59593-186-4/06/0007}

\title{Particle Swarm Optimization for Crystallography}
\numberofauthors{4}
%\author{%
%\alignauthor Matthew Gardner\\
%    \affaddr{Brigham Young University}\\
%    \affaddr{Computer Science Dept.}\\
%    \affaddr{3361 TMCB, Provo, UT 84602}\\
%    \email{mjg82@cs.byu.edu}
%\alignauthor Andrew McNabb\\
%    \affaddr{Brigham Young University}\\
%    \affaddr{Computer Science Dept.}\\
%    \affaddr{3361 TMCB, Provo, UT 84602}\\
%    \email{a@cs.byu.edu}
%\alignauthor Branton Campbell\\
%    \affaddr{Brigham Young University}\\
%    \affaddr{Physics Dept.}\\
%    \affaddr{N261 ESC, Provo, UT 84602}\\
%    \email{branton_campbell@byu.edu}
%\alignauthor Kevin Seppi\\
%    \affaddr{Brigham Young University}\\
%    \affaddr{Computer Science Dept.}\\
%    \affaddr{3361 TMCB, Provo, UT 84602}\\
%    \email{k@cs.byu.edu}
%}
\date{}

\author{Author names omitted for review}

\maketitle

\begin{abstract}
TODO.
\end{abstract}

\section*{Track Category}
FIXME. Ant Colony Optimization, Swarm Intelligence

\category{FIXME. G.1.6}{Numerical Analysis}{Optimization}[\\ nonlinear programming, unconstrained optimization]

\terms{FIXME. Algorithms}

\keywords{FIXME. Swarm intelligence, Parallelization, Optimization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction

\section{Introduction}
\label{sec:intro}

TODO.
Particle swarm optimization is most often used with functions that are quickly
evaluated.  Many real-life problems take much longer.  What does one do when 
presented with a function that takes several minutes or hours to evaluate?  

In this paper we present a crystallography problem whose function evaluation
takes several minutes to evaluate.  We discuss issues in solving this problem
specifically, and in optimizing expensive functions in general with particle 
swarm optimization.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Crystallography

\section{Crystallography}
\label{sec:crystallography}

FIXME.
TODO: Where to put the butterfly problem definition? Should I describe 
simulated annealing or gradient descent, at least briefly?

The point of crystallography is to determine the structure of a crystal.  This
is done by shining x-rays through the crystal and observing the patterns that
come out.  Those patterns are then compared to the output of a model of the 
crystal -- if the two match, the model can be considered correct.  The model
may consist of the positions of all of the atoms in the crystal, the 
distortion coefficients of the materials, the length and width of nanopolar
domains, or various other parameters.  The task of finding the parameters to 
the model that best fit the data is well suited to optimization techniques.  
The function to be optimized is an error function -- the average difference
between the model output and the data, for every data point.

There are commercial packages that readily solve some crystallography problems,
such as (ask Dr. Campbell), that uses simulated annealing for specific types 
of problems.  Gradient descent is also a commonly used method for solving these
problems.  Particle swarm optimization, however (to our knowledge?), has not 
been used.  

We encountered a problem for which gradient descent proved uneffective.  

To solve crystallography problems, one must fit the parameters of the model
to the data.  The more data is used, the more accurate the result will be, but
with large amounts of data the evaluation time can be inhibitively large.  In
our first attempts at solving this problem, the function evaluation took close
to an hour.  We then trimmed the amount of data we were using, and the time for
a function evaluation decreased to about four minutes.  



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% PSO

\section{Particle Swarm Optimization}
\label{sec:pso}

TODO: Matt, you probably want to put this in your own words, but if we
really run out of time, you don't really have to.

Particle Swarm Optimization simulates the motion of particles in the domain of
a function.  These particles search for the optimum by evaluating the function
as they move.  During each iteration of the algorithm, the position and
velocity of each particle are updated.  Each particle is pulled toward the
best position it has sampled (personal best) and the best position of any
particle in its neighborhood (global best).  This attraction is weak enough to
allow exploration but strong enough to encourage exploitation of good
locations and to guarantee convergence.

Each particle's position and velocity are initialized to random values based
on a function-specific feasible region.  During each iteration of constricted
PSO, the following equations update a particle's position $\ppos$ and velocity
$\pvel$ with respect to personal best $\pbest$ and global best $\gbest$:
\begin{align}
\label{eq:velupdate}
    \pvel_{t+1} &=
        \constriction \left[ \pvel_t + 
            \coeff_1\URand()\otimes(\pbest - \ppos_{t}) +
            \coeff_2\URand()\otimes(\gbest - \ppos_{t})
        \right] \\
\label{eq:posupdate}
    \ppos_{t+1} &= \ppos_t + \pvel_{t+1}
\end{align}
where $\coeff_1 = \coeff_2 = 2.05$, $\URand()$ is a vector of samples
drawn from a standard uniform distribution, and $\otimes$ represents
element-wise multiplication.  The constriction constant $\constriction$
is:
\begin{align}
    \notag
    \constriction &= \frac{2 \kappa}{|2 - \coeff - \sqrt{\coeff^2 - 4\coeff}|}
\end{align}
where
$\coeff = \coeff_1 + \coeff_2$ and $\kappa$ is usually
$1.0$~\cite{clerc-tec02}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MRPSO


\section{MapReduce}
\label{sec:mapreduce}

TODO: Matt, you probably want to put this in your own words, but if we
really run out of time, you don't really have to.  This should also be trimmed
down.

MapReduce is a functional programming model that is well suited to parallel
computation.  In the model, a program consists of a map function and a reduce
function which meet a few simple requirements.  These functions are only
concerned with the high-level task of processing input and returning output.
For formulating a problem in terms of a map function and a reduce function, a
programmer is rewarded with automatic parallelization, with communication,
load balancing, fault tolerance, and all other details managed by the
MapReduce implementation.

In MapReduce, all data are in the form of keys with associated values.  For
example, in a program that counts the frequency of occurrences for various
words, the key would be a word and the value would be its frequency.

A MapReduce operation takes place in two main stages.  In the first stage, the
map function is called once for each input record.  At each call, it may
produce any number of output records.  In the second stage, this intermediate
output is sorted and grouped by key, and the reduce function is called once
for each key.  The reduce function is given all associated values for the
key and outputs a new list of values (often ``reduced'' in length from the
original list of values).

The following notation and example are based on the original
presentation~\cite{dean-osdi04}.

\subsection{Map Function}

A map function is defined as a function that takes a single key-value pair and
outputs a list of new key-value pairs.  The input key may be of a different
type than the output keys, and the input value may be of a different type than
the output values:
\begin{align}
\mathsf{map}:\:&(\InputKeySet, \InputValSet) \rightarrow
    \List{(\OutputKeySet, \OutputValSet)}
\end{align}

Since the map function only takes a single record, all map operations are
independent of each other and fully parallelizable.

\subsection{Reduce Function}

A reduce function is a function that reads a key and a corresponding list of
values and outputs a new list of values for that key.  The input and output
values are of the same type.  Mathematically, this would be written:
\begin{align}
\mathsf{reduce}:\:&\Pair{\OutputKeySet}{\List{\OutputValSet}} \rightarrow
\List{\OutputValSet}
\end{align}

A reduce operation may depend on the output from any number of map calls, so
no reduce operation can begin until all map operations have completed.
However, the reduce operations are independent of each other and may be run in
parallel.

Although the formal definition of map and reduce functions would indicate
building up a list of outputs and then returning the list at the end, it is
more convenient in practice to emit one element of the list at a time and
return nothing.  Conceptually, these emitted elements still constitute a list.

\subsection{Example: WordCount}

The classic MapReduce example is WordCount, a program which counts the number
of occurrences of each word in a document or set of documents.  For this
program, the input and output sets are:
\begin{align}
\InputKeySet:\: &\NaturalSet
\label{eq:wc-inputkey}
\\
\InputValSet:\: & \text{set of all strings}
\label{eq:wc-inputval}
\\
\OutputKeySet:\: &\text{set of all strings}
\label{eq:wc-outputkey}
\\
\OutputValSet:\: &\NaturalSet
\label{eq:wc-outputval}
\end{align}

In WordCount, the input value is a line of text.  The input key is ignored but
arbitrarily set to be the line number for the input value.  The output key is
a word, and the output value is its count.

\begin{algorithm}[tbp]
\caption{WordCount Map}
\label{alg:wc-map}
\begin{lstlisting}[language=Python]
def mapper(key, value):
    for word in value.split():
        emit((word, 1))
\end{lstlisting}
\end{algorithm}

\begin{algorithm}[tbp]
\caption{WordCount Reduce}
\label{alg:wc-reduce}
\begin{lstlisting}[language=Python]
def reducer(key, value_list):
    total = sum(value_list)
    emit(total)
\end{lstlisting}
\end{algorithm}

The map function, shown as Function~\ref{alg:wc-map}, splits the input line
into individual words.  For each word, it emits the key-value pair formed by
the word and the value 1.

The reduce function, shown as Function~\ref{alg:wc-reduce}, takes a word and
list of counts, performs a sum reduction, and emits the result.  This is the
only element emitted, so the output of the reduce function is a list of size
1.

These map and reduce functions are deceptively simple.  The problem itself is
inherently difficult---implementing a scalable distributed word count system
with fault-tolerance and load-balancing is not easy.  However all of the
complexity is found in the surrounding MapReduce infrastructure rather than in
the map and reduce functions.  Note that the reduce function does not even
output a key, since the MapReduce system already knows what key it passed in.

The data given to map and reduce functions, as in this example, are generally
as fine-grained as possible.  This ensures that the implementation can split
up and distribute tasks.  The MapReduce system consolidates the intermediate
output from all of the map tasks.  These records are sorted and grouped by
key before being sent to the reduce tasks.

If the map tasks emit a large number of records, the sort phase can take a
long time.  MapReduce addresses this potential problem by introducing the
concept of a combiner function.  If a combiner is available, the MapReduce
system will locally sort the output from several map calls on the same machine
and perform a ``local reduce'' using the combiner function.  This reduces the
amount of data that must be sent over the network for the main sort leading to
the reduce phase. In WordCount, the reduce function would work as a combiner
without any modifications.

\subsection{Benefits of MapReduce}

Although not all algorithms can be efficiently formulated in terms of map and
reduce functions, MapReduce provides many benefits over other parallel
processing models.  In this model, a program consists of only a map function
and a reduce function.  Everything else is common to all programs.  The
infrastructure provided by a MapReduce implementation manages all of the
details of communication, load balancing, fault tolerance, resource
allocation, job startup, and file distribution.  This runtime system is
written and maintained by parallel programming specialists, who can ensure
that the system is robust and optimized, while those who write mappers and
reducers can focus on the problem at hand without worrying about
implementation details.

A MapReduce system determines task granularity at runtime and
distributes tasks to compute nodes as processors become available.  If some
nodes are faster than others, they will be given more tasks, and if a node
fails, the system automatically reassigns the interrupted task.

\subsection{MapReduce Implementations}

Google has described its MapReduce implementation in published papers and
slides, but it has not released the system to the public.  Presumably the
implementation is highly optimized because Google uses it to produce its web
index.

The Apache Lucene project has developed Hadoop, an Java-based open-source
clone of Google's closed MapReduce platform.  The platform is relatively new
but rapidly maturing.  At this time, Hadoop overhead is significant but not
overwhelming, and is expected to decrease with further development.


\section{MapReduce PSO (MRPSO)}
\label{sec:mrpso}

%(1, ``1,2,3;1.3,-0.5;-0.8,0.4;1.94;0.9,0.7;1.3;-0.2,1.0;1.04")
\begin{figure*}
\begin{align*}
(3, ``1,2,3,4;1.7639,2.5271;52.558,50.444;9.4976;1.7639,2.5271;9.4976;-1.0151,-2.0254;5.1325")
\end{align*}
\caption{A particle as a key-value pair}
\label{fig:sample-particle}
\end{figure*}

\begin{algorithm*}[tbp]
\caption{MRPSO Map}
\label{alg:pso-map}
\begin{lstlisting}[language=Python]
def mapper(key, value):
    particle = Particle(value)

    # Update the particle:
    new_position, new_velocity = pso_motion(particle)
    y = evaluate_function(new_position)
    particle.update(new_position, new_velocity, y)

    # Emit a message for each dependent particle:
    message = particle.make_message()
    for dependent_id in particle.dependent_list:
        if dependent_id == key:
            particle.gbest_candidate(particle.pbest_position, particle.pbest_value)
        else:
            emit((dependent_id, repr(message)))

    # Emit the updated particle without changing its id:
    emit((key, repr(particle)))
\end{lstlisting}
\end{algorithm*}

\begin{algorithm*}[tbp]
\caption{MRPSO Reduce}
\label{alg:pso-reduce}
\begin{lstlisting}[language=Python]
def reducer(key, value_list):
    particle = None
    best = None

    # Of all of the inputs, find the record with the best gbest_value:
    for value in value_list:
        record = Particle(value)
        if (best is None) or (record.gbest_value <= best.gbest_value):
            best = record
        if not record.is_message():
            particle = record

    # Update the gbest of the particle and emit:
    if particle is not None:
        particle.gbest_candidate(best.gbest_position, best_value)
        emit(repr(particle))
    else:
        emit(repr(best))
\end{lstlisting}
\end{algorithm*}

% The first sentence here is still broken:

In an iteration of Particle Swarm Optimizations, each particle in the swarm
moves to a new position, updates its velocity, evaluates the function at the
new point, updates its personal best if this value is the best seen so far,
and updates its global best after comparison with its neighbors.  Except for
updating its global best, each particle updates independently of the rest of
the swarm.

Due to the limited communication among particles, updating a swarm can be
formulated as a MapReduce operation.  As a particle is mapped, it receives a
new position, velocity, value, and personal best.  In the reduce phase, it
incorporates information from other particles in the swarm to update its
global best.  The MRPSO implementation conforms to the MapReduce model while
performing the same calculations as standard Particle Swarm Optimization.

\subsection{Particle Representation and Messages}

In MRPSO, the input and output sets are:
\begin{align}
\InputKeySet:\: &\NaturalSet
\label{eq:pso-inputkey}
\\
\InputValSet:\: & \text{set of all strings}
\label{eq:pso-inputval}
\\
\OutputKeySet:\: &\NaturalSet
\label{eq:pso-outputkey}
\\
\OutputValSet:\: &\text{set of all strings}
\label{eq:pso-outputval}
\end{align}

Each particle is identified by a numerical \verb$id$ key, and particle state
is represented as a string.  The state of a particle consists of its
dependents list (neighbors' \verb$id$s), position, velocity, value, personal
best position, personal best value, global best position, and global best
value.  The state string is a semicolon-separated list of fields.  If a field
is vector valued, its individual elements are comma-separated.  The state
string is of the form:
\begin{align*}
\text{deps};\text{pos};\text{vel};\text{val};\text{pbpos};\text{pbval};\text{gbpos};\text{gbval}
\end{align*}

A typical particle is shown in Figure~\ref{fig:sample-particle}.  This
particle is exploring the function $f(\Vec{x}) = x_1^2 + x_2^2$.  Its
components are interpreted as follows:

\begin{tabbing}
$3$ \hspace{72pt} \= particle id \\
$1,2,3,4$ \> dependents (neighbors) \\
$1.7639,2.5271$ \> current position $(x_1, x_2)$ \\
$52.558,50.444$ \> velocity $(x_1, x_2)$ \\
$9.4976$ \> value of $f(\Vec{x})$ at the current position \\
\> \hspace{16pt} ($1.7639^2 + 2.5271^2$) \\
$1.7639,2.5271$ \> personal best position $(x_1, x_2)$ \\
$9.4976$ \> personal best value \\
$-1.0151,-2.0254$ \> global best position $(x_1, x_2)$ \\
$5.1325$ \> global best value \\
\end{tabbing}

MRPSO also creates messages, which are like particles except that they have
empty dependents lists.  A message is sent from one particle to another as
part of the MapReduce operation.  In the reduce phase, the recipient reads the
personal best from the message and updates its global best accordingly.

\subsection{MRPSO Map Function}

The MRPSO map function, shown as Function~\ref{alg:pso-map}, is called once
for each particle in the population.  The key is the \verb$id$ of the
particle, and the value is its state string representation.  The PSO mapper
finds the new position and velocity of the particle and evaluates the function
at the new point.  It then calls the update method of the particle with this
information.  In addition to modifying the particle's state to reflect the new
position, velocity, and value, this method replaces the personal best if a
more fit position has been found.

The key to implementing PSO in MapReduce is communication between particles.
Each particle maintains a dependents list containing the \verb$id$s of all
neighbors that need information from the particle to update their own global
bests.  After the map function updates the state of a particle, it emits
messages to all dependent particles.  When a message is emitted, its
corresponding key is the \verb$id$ of the destination particle, and its value
is the string representation, which includes the position, velocity, value,
and personal best of the source particle.  The global best of the message is
also set to the personal best.

If the particle is a dependent of itself, as is usually the case, the map
function updates the global best of the particle if the personal best is an
improvement.  Finally, the map function emits the updated
particle and terminates.

\subsection{MRPSO Reduce Function}

The MRPSO reduce function, shown as Function~\ref{alg:pso-reduce}, receives a
key and a list of all associated values.  The key is the \verb$id$ of a
particular particle in the population.  The values list contains the newly
updated particle and a message from each neighbor.  The PSO reducer combines
information from all of these messages to update the global best position and
global best value of the particle.  The reducer emits only the updated
particle.

Function~\ref{alg:pso-reduce} also works as a combiner.  If no particle is
found in the input value list, the function combines the list by emitting only
the best message.  This message is then sent to the reducer.

\subsection{Map and Reduce in Context}

When a particle is emitted by a reducer, it is fully updated.  In the map
phase, it updated its velocity, moved to a new position, evaluated the
function at that position, and updated its personal best.  In the reduce
phase, it updated its global best after receiving messages from all of its
neighbors in the swarm.  A map phase followed by a reduce phase performs an
iteration of the swarm that is exactly equivalent to an iteration in
single-processor PSO.

Observe that the MRPSO implementation does not explicitly deal with
communication across nodes, load balancing, or node failures.  The MapReduce
formulation of the problem allows the work to be divided in small enough
pieces that the MapReduce system can balance work across processors and deal
with failed nodes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography

\bibliographystyle{plain}
\bibliography{%
    \bib{mapreduce/dean-osdi04},%
    \bib{pso/clerc-tec02},%
    \bib{pso/kennedy-icnn95},%
    \bib{pso/bratton-sis07},%
    \bib{aml/mcnabb-cec07}}


\end{document}
% vim: lbr et sw=4 sts=4

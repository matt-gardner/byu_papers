\documentclass{beamer}
\usepackage{beamerthemeshadow}
\beamertemplatetransparentcovereddynamic
\usepackage{graphicx}

\author{Matthew Gardner}
\title{LDA with Gibbs Sampling Revisted}
\date{April 10, 2009}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{LDA}

\subsection{Overview}

\begin{frame}
  \frametitle{My Intent}
  \begin{itemize}
	\item Regular old LDA with Gibbs sampling
	  \pause
	\item I basically tried to reproduce the Griffiths and Steyvers paper
  \end{itemize}
\end{frame}

\subsection{Review of LDA and Gibbs}

\begin{frame}
  \frametitle{LDA}
  \begin{itemize}
	\item Find latent topics in a data set
	  \begin{align*} 
		w_i|z_i,\phi^{(z_i)} &\sim \mathrm{Categorical}(\phi^{(z_i)}) \\
		\phi &\sim \mathrm{Dirichlet}(\beta) \\
		z_i|\theta^{(d_i)} &\sim \mathrm{Categorical}(\theta^{(d_i)}) \\ 
		\theta &\sim \mathrm{Dirichlet}(\alpha) 
	  \end{align*}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Gibbs Sampling}
  \begin{itemize}
	\item For one sample:
	  \begin{itemize}
		\item For each token:
		\item Create a distribution $p(z_i=j|\mathbf{z}_{-i},\mathbf{w})$
		  according to:
		  \[p(z_i = j|\mathbf{z}_{-i},\mathbf{w}) \propto 
		  \frac{n_{-i,j}^{(w_i)}+\beta}{n_{-i,j}^{(\cdot)}+W\beta}
		  \frac{n_{-i,j}^{(d_i)}+\alpha}{n_{-i,\cdot}^{(d_i)}+T\alpha}\]
		\item Draw from the distribution and reassign $z_i$, being sure to 
		  keep good track of your counts.
	  \end{itemize}
	\item Do this for a number of samples as a burn in period, then take however
	  many samples you want, with a lag in between.
  \end{itemize}
\end{frame}

\subsection{Data Sets}

\begin{frame}
  \frametitle{Data Sets}
  \begin{itemize}
	\item LDS General Conference talks from 1900-2005
	  \pause
	  \begin{itemize}
		\item Somewhat noisy due to parsing error
	  \end{itemize}
	  \pause
	\item 20 Newsgroups, full data set
	  \pause
	  \begin{itemize}
		\item Also somewhat noisy due to parsing error, but a different kind
	  \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Feature Selection}
  \begin{itemize}
	\item Top 5000 words
	  \pause
	\item That gave about 15,000,000 tokens, most of which were meaningless
	  \pause
	\item I made a stop words list by looking at the top 70 words in the 
	  data---down to about 7,000,000 tokens
  \end{itemize}
\end{frame}

\section{General Conference Results}

\begin{frame}
  \frametitle{General Conference Data}
\end{frame}

\subsection{How many topics?}

\begin{frame}
  \frametitle{How Many Topics?}
  \pause
  \begin{itemize}
	\item Newton and Raftery estimate:
	  \[\hat{p}(\mathbf{w}) = \left\{\frac{1}{G}\sum_{g=1}^{G}\left(\frac{1}
	  {p(\mathbf{w}|\theta^{(g)},M_k)}\right)\right\}^{-1}\]
	  where each $\theta^{(g)}$ is a draw from $p(\theta|\mathbf{w})$
	\item In log space, this turns into:
	  \[\log\left(\hat{p}(\mathbf{w})\right) = -1\left(-\log(G)+
	  \mathrm{logSum}_{g=1}^G(-\log\left(p(\mathbf{w}|\theta_g)\right)\right)\]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{100 Topics}
  \begin{figure}
	\includegraphics[width=.5\textwidth]{likelihoodgc.eps}
  \end{figure}
  \begin{itemize}
	\item I didn't try more than 100 topics, because of time constraints, but
	  of the ones that I tried, 100 topics had the highest likelihood.
  \end{itemize}
\end{frame}
\subsection{Topic-Word distributions}

\begin{frame}
  \frametitle{Topic-Word distributions}
\end{frame}

\begin{frame}
  \frametitle{Topic-Word distributions}
  \begin{tabular}{|c|c|c|}
	Topic 1: Good and Evil &\ \ \ \ \ \ \ \ \ \ \ \ \ \ &Topic 7: Government \\
	evil&&government \\
	against&&united \\
	satan&&liberty \\
	good&&nation \\
	sin&&states \\
	devil&&country \\
	must&&freedom \\
	power&&land \\
	destroy&&free \\
	god&&men \\
	righteousness&&constitution \\
	temptation&&people \\
	away&&america \\
  \end{tabular}
\end{frame}
\begin{frame}
  \frametitle{Topic-Word distributions}
  \begin{tabular}{|c|c|c|}
	Topic 8: Questions &\ \ \ \ \ \ \ \ \ \ \ &Topic 22: Character \\
	?&&true \\
	how&&honest \\
	why&&good \\
	question&&integrity \\
	then&&virture \\
	answer&&character \\
	did&&honesty \\
	asked&&without \\
	ask&&one \\
	yes&&men \\
	where&&honor \\
	many&&respect \\
	could&&god \\
  \end{tabular}
\end{frame}
\begin{frame}
  \frametitle{Topic-Word distributions}
  \begin{tabular}{|c|c|c|}
	Topic 26: Numbers &\ \ \ \ \ \ \ \ \ \ \ &Topic 28: Numbers \\
	see&&years \\
	d&&one \\
	c&&two \\
	1&&hundred \\
	2&&five \\
	3&&three \\
	s&&thousand \\
	ne&&twenty \\
	4&&ago \\
	5&&four \\
	alma&&year \\
	6&&six \\
	7&&over \\
  \end{tabular}
\end{frame}
\begin{frame}
  \frametitle{Topic-Word distributions}
  \begin{tabular}{|c|c|c|}
	Topic 33: Welfare &\ \ \ \ \ \ \ \ \ \ \ &Topic 19: Tithing \\
	welfare&&tithing \\
	poor&&pay \\
	church&&money \\
	fast&&debt \\
	help&&means \\
	services&&out \\
	program&&people \\
	plan&&one \\
	care&&paid \\
	needs&&poor \\
	food&&tithes \\
	need&&get \\
	family&&more \\
  \end{tabular}
\end{frame}
\begin{frame}
  \frametitle{Topic-Word distributions}
  \begin{tabular}{|c|c|c|}
	Topic 37: Families &\ \ \ \ \ \ \ \ \ \ \ &Topic 52: Women \\
	children&&women \\
	home&&society \\
	family&&relief \\
	parents&&sisters \\
	teach&&hinckley \\
	child&&each \\
	families&&b \\
	homes&&gordon \\
	love&&lives \\
	mother&&service \\
	taught&&sister \\
	fathers&&forward \\
	responsibility&&strength \\
  \end{tabular}
\end{frame}

\subsection{Topic-Speaker distributions}

\begin{frame}
  \frametitle{Topic-Speaker distributions}
\end{frame}

\begin{frame}
  \frametitle{Topic-Speaker distributions}
  \begin{tabular}{|c|c|c|}
	Topic 52: Women &\ \ \ \ \ \ \ \ \ \ \ &Topic 52 Speakers \\
	women&&Sister Barbara B. Smith \\
	society&&Mary Ellen W. Smoot \\
	relief&&Mary Ellen Smoot \\
	sisters&&Elaine L. Jack \\
	hinckley&&Marian R. Boyer \\
	each&&Virginia U. Jensen \\
	b&&Wayne M. Hancock \\
	gordon&&Shirley W. Thomas \\
	lives&&Bonnie D. Parkin \\
	service&&Kathleen H. Hughes \\
	sister&&Sheri L. Dew \\
	forward&&Richard H. Winkel \\
	strength&&Aileen H. Clyde \\
  \end{tabular}
\end{frame}
\begin{frame}
  \frametitle{Topic-Speaker distributions}
  \begin{tabular}{|c|c|c|}
	Topic 61&&Topic 61 Speakers \\
	school&&Second Assistant Karl G. Maeser \\
	sunday&&First Assistant Superintendent George Reynolds \\
	schools&&E. C. Phillips \\
	education&&Josiah H. Burrows \\
	children&&First Assistant General Superintendent George Goddard\\
	teachers&&Supt. Joseph J. Jackson of the Alpine Stake\\
	class&&L. John Nuttall \\
	teacher&&Deseret Sunday School Union \\
	students&&Supt. John D. Peters \\
	training&&Supt. Joseph J. Jackson \\
	university&&General Superintendent George Q. Cannon \\
	religion&&Apostle Heber J. Grant \\
	some&&Joseph W. Summerhays \\
  \end{tabular}
\end{frame}


\subsection{Topics over time}

\begin{frame}
  \frametitle{Topics over time}
\end{frame}

\begin{frame}
  \frametitle{Topics over time}
  \begin{figure}
	\includegraphics[width=.8\textwidth]{topic1.eps}
  \end{figure}
\end{frame}
\begin{frame}
  \frametitle{Topics over time}
  \begin{figure}
	\includegraphics[width=.8\textwidth]{topic19.eps}
  \end{figure}
\end{frame}
\begin{frame}
  \frametitle{Topics over time}
  \begin{figure}
	\includegraphics[width=.8\textwidth]{topic33.eps}
  \end{figure}
\end{frame}
\begin{frame}
  \frametitle{Topics over time}
  \begin{figure}
	\includegraphics[width=.8\textwidth]{topic37.eps}
  \end{figure}
\end{frame}
\begin{frame}
  \frametitle{Topics over time}
  \begin{figure}
	\includegraphics[width=.8\textwidth]{topic52.eps}
  \end{figure}
\end{frame}
\begin{frame}
  \frametitle{Topics over time}
  \begin{figure}
	\includegraphics[width=.8\textwidth]{topic56.eps}
  \end{figure}
\end{frame}

\subsection{Quantitative Results}

\begin{frame}
  \frametitle{Perplexity}
  \begin{itemize}
	\item Only one good quantitative metric---perplexity
	  \pause
	\item Intuitively, it is the uncertainty in a model for predicting a single
	  word.
	  \pause
	\item Formally, perplexity is:
	  \[Perp(D) = \prod_{m=1}^{|D|}p(d_i)^{-\frac{1}{N_m}}
	  = \exp\left(-\frac{\sum_{m=1}^{|D|}\log p(d_i)}{\sum_{m=1}^{|D|}N_m}
	  \right)\]
	  \[p(d_i) = \prod_{n=1}^{N_m}\sum_{k=1}^{K}p(w_n=t|z_n=k)p(z_n=k|d_i)\]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Perplexity}
  \begin{itemize}
	\item Supposed to have held out data, but LDA doesn't allow for that so
	  much.
	\item I picked 500 random documents to test perplexity on.
	  \begin{tabular}{|c|c|}
		\hline
		Number of Topics&Perplexity \\
		\hline
		10&1320.4 \\
		\hline
		20&1245.0 \\
		\hline
		30&1202.3 \\
		\hline
		40&1179.6 \\
		\hline
		50&1165.5 \\
		\hline
		100&1125.3 \\
		\hline
	  \end{tabular}
	\item Unigram perplexity: 1673.6
  \end{itemize}
\end{frame}






\section{20 Newsgroups Results}

\begin{frame}
  \frametitle{20 Newsgroups data set}
\end{frame}

\subsection{How many topics?}

\begin{frame}
  \frametitle{How many topics?}
\end{frame}

\begin{frame}
  \frametitle{50 Topics}
  \begin{figure}
	\includegraphics[width=.5\textwidth]{likelihoodgroups.eps}
  \end{figure}
  \begin{itemize}
	\item Again, I didn't try more than 50, and 50 did best.
  \end{itemize}
\end{frame}

\subsection{Topic-Word distributions}
\begin{frame}
  \frametitle{Topic-Word distributions}
  \begin{tabular}{|c|c|c|}
	Topic 0: Hardware&\ \ \ \ \ \ \ \ \ \ \ \ \ \ &Topic 15: Space \\
	drive&&space \\
	card&&scispace \\
	scsi&&nasa \\
	system&&earth \\
	disk&&launch \\
	mac&&system \\
	compsysibmpchardware&&its \\
	pc&&new \\
	compsysmachardware&&orbit \\
	video&&center \\
	dos&&mission \\
	windows&&moon \\
	hard&&shuttle \\
  \end{tabular}
\end{frame}
\begin{frame}
  \frametitle{Topic-Word distributions}
  \begin{tabular}{|c|c|c|}
	Topic 31: Quoting&\ \ \ \ \ \ \ \ \ \ \ \ \ \ &Topic 13: Middle East \\
	writes&&israel \\
	article&&jews \\
	re&&war \\
	$>$the&&israeli \\
	references&&jewish \\
	$>$i&&talkpoliticsmideast \\
	$>$in&&its \\
	?&&arab \\
	think&&peace \\
	like&&world \\
	$>$and&&people \\
	$>$to&&american \\
	$>$of&&land \\
  \end{tabular}
\end{frame}

\subsection{``Confusion Matrix''}

\begin{frame}
  \frametitle{``Confusion Matrix''}
  \begin{tabular}{|c|c|c|c|c|c|}
	\hline
	comp sys ibm&\bf{.2065}&\bf{.2065}&.0117&.0243&.0186 \\
	\hline
	comp sys mac&\bf{.1628}&\bf{.1628}&.0079&.0261&.0231 \\
	\hline
	comp os ms-windows&\bf{.0621}&\bf{.0621}&\bf{.1817}&.0386&\bf{.0580} \\
	\hline
	comp graphics&.0281&.0281&.0112&\bf{.1781}&\bf{.1336} \\
	\hline
	comp windows x&.0208&.0208&.0204&\bf{.0847}&\bf{.2221} \\
	\hline
	Topic number&0&0&23&34&28 \\
	\hline
  \end{tabular}
\end{frame}


\subsection{Quantitative Results}

\begin{frame}
  \frametitle{Perplexity}
  \begin{itemize}
	\item Supposed to have held out data, but LDA doesn't allow for that so
	  much.
	\item I picked 500 random documents to test perplexity on.
	  \begin{tabular}{|c|c|}
		\hline
		Number of Topics&Perplexity \\
		\hline
		10&1142.7 \\
		\hline
		30&997.2 \\
		\hline
		40&957.5 \\
		\hline
		50&943.2 \\
		\hline
	  \end{tabular}
	\item Unigram perplexity: 1736.8
  \end{itemize}
\end{frame}
\end{document}

\documentclass[ms]{byuprop}
% Options for this class include the following (* indicates default):
%
%   10pt -- 10 point font size
%   11pt -- 11 point font size
%   12pt (*) -- 12 point font size
%
%   ms -- produce a thesis proposal (off)
%   areaexam -- produce a research area overview (off)
%   phd -- produce a dissertation proposal (off)
%
%   layout -- show layout lines on the pages, helps with overfull boxes (off)
%   grid -- show a half-inch grid on every page, helps with printing (off)


% This command fixes my particular printer, which starts 0.03 inches too low,
% shifting the whole page down by that amount.  This shifts the document
% content up so that it comes out right when printed.
%
% Discovering this sort of behavior is best done by specifying the ``grid''
% option in the class parameters above.  It prints a 1/2 inch grid on every
% page.  You can then use a ruler to determine exactly what the printer is
% doing.
%
% Uncomment to shift content up (accounting for printer problems)
%\setlength{\voffset}{-.03in}

% Here we set things up for invisible hyperlinks in the document.  This makes
% the electronic version clickable without changing the way that the document
% prints.  It's useful, but optional.
\usepackage[
    ps2pdf,
    bookmarks=true,
    breaklinks=true,
    raiselinks=true,
    pdfborder={0 0 0},
    colorlinks=false,
    ]{hyperref}

% Rewrite the itemize, description, and enumerate environments to have more
% reasonable spacing:
\newcommand{\ItemSep}{\itemsep 0pt}
\let\oldenum=\enumerate
\renewcommand{\enumerate}{\oldenum \ItemSep}
\let\olditem=\itemize
\renewcommand{\itemize}{\olditem \ItemSep}
\let\olddesc=\description
\renewcommand{\description}{\olddesc \ItemSep}

% Get a little less fussy about word spacing on a line.  Sometimes produces
% ugly results, so keep your eyes peeled.
\sloppy

% Important settings for the byuprop class. %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Because I use these things in more than one place, I created new commands for
% them.  I did not use \providecommand because I absolutely want LaTeX to error
% out if these already exist.
\newcommand{\Title}{A Speculative Approach to Parallelization in Particle Swarm
Optimization}
\newcommand{\Author}{Matthew Gardner}
\newcommand{\SubmissionMonth}{April}
\newcommand{\SubmissionYear}{2011}

% Take these from the commands defined above
\title{\Title}
\author{\Author}
\monthsubmitted{\SubmissionMonth}
\yearsubmitted{\SubmissionYear}

% Committee members
\committeechair{Kevin Seppi}
\committeemembera{Dan Ventura}
\committeememberb{David Embley}
%\committeememberc{}
%\committeememberd{}

% Department graduate coordinator
\graduatecoordinator{Kent Seamons}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Set up the internal PDF information so that it becomes part of the document
% metadata.  The pdfinfo command will display this. Be sure to set the document
% type and add your own keywords.
\hypersetup{%
    pdftitle=\Title,%
    pdfauthor=\Author,%
    pdfsubject={Masters thesis proposal, BYU CS Department: %
                Submitted \SubmissionMonth~\SubmissionYear, Created \today},%
    pdfkeywords={parallel algorithms, optimization methods, particle swarm %
				optimization, speculative decomposition},%
}

% These packages allow the bibliography to be sorted alphabetically and allow references to more than one paper to be sorted and compressed (i.e. instead of [5,2,4,6] you get [2,4-6])
\usepackage{url}
\usepackage[numbers,sort&compress]{natbib}
\bibliographystyle{annotnat}
\usepackage{hypernat}
% Note, use \citet to name the authors (text mode) and \citep to do just give
% the number (parenthetical mode).  Also, \citep* will give all of the authors
% rather than an abbreviated list.

% Additional packages required for your specific thesis go here. I've left some I use as examples.
\usepackage{graphicx}
\usepackage{psfrag}
%\usepackage{pdfsync}
\usepackage{amsmath}

% Better references, I think
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}

%% PSO Stuff
\DeclareMathOperator*{\argmin}{arg\;min}
\DeclareMathOperator*{\argmax}{arg\;max}
\DeclareMathOperator*{\arginf}{arg\;inf}
\DeclareMathOperator*{\argsup}{arg\;sup}
\providecommand{\pers}{\ensuremath{P}}
\providecommand{\neigh}{\ensuremath{N}}
\providecommand{\leftind}{\ensuremath{L}}
\providecommand{\rightind}{\ensuremath{R}}
\providecommand{\nURand}{\ensuremath{U^\neigh}}
\providecommand{\pURand}{\ensuremath{U^\pers}}
\providecommand{\ppos}{\ensuremath{\Vec{x}}}
\providecommand{\fppos}{\ensuremath{\Vec{fx}}}
\providecommand{\pvel}{\ensuremath{\Vec{v}}}
\providecommand{\nbest}{\ensuremath{\Vec{b}^\neigh}}
\providecommand{\fnbest}{\ensuremath{\Vec{fb}^\neigh}}
\providecommand{\pbest}{\ensuremath{\Vec{b}^\pers}}
\providecommand{\fpbest}{\ensuremath{\Vec{fb}^\pers}}
\providecommand{\constriction}{\ensuremath{\chi}}
\providecommand{\ncoeff}{\ensuremath{\phi^\neigh}}
\providecommand{\pcoeff}{\ensuremath{\phi^\pers}}
\providecommand{\obs}{\ensuremath{\Vec{\xi}}}
\providecommand{\ofunc}{\ensuremath{f}}
\providecommand{\swarm}{\ensuremath{swarm}}

%SpecExPSO Stuff
\providecommand{\indic}{\ensuremath{I}}
\providecommand{\specvel}{\ensuremath{\vec{V}}}
\providecommand{\specpos}{\ensuremath{\vec{X}}}
\providecommand{\leftn}{\ensuremath{\Vec{x}^\leftind}}
\providecommand{\rightn}{\ensuremath{\Vec{x}^\rightind}}
\providecommand{\caseset}{\ensuremath{\mathcal{C}}}
\providecommand{\casegen}{\ensuremath{c}}
\providecommand{\casedef}{\ensuremath{(\pbest,\nbest)}}
\providecommand{\casexn}{\ensuremath{(S,-)}}
\providecommand{\casexx}{\ensuremath{(S,S)}}
\providecommand{\casexl}{\ensuremath{(S,\leftind)}}
\providecommand{\casexr}{\ensuremath{(S,\rightind)}}
\providecommand{\casepn}{\ensuremath{(-,-)}}
\providecommand{\casepl}{\ensuremath{(-,\leftind)}}
\providecommand{\casepr}{\ensuremath{(-,\rightind)}}
\providecommand{\casepN}{\ensuremath{(-,N)}}
\providecommand{\casexN}{\ensuremath{(S,N)}}
\providecommand{\noeval}[1]{\ensuremath{#1^{-e}}}
\providecommand{\nonbest}[1]{\ensuremath{#1^{-n}}}
\providecommand{\p}{\ensuremath{p}}
\providecommand{\pset}{\ensuremath{\mathbf{p}}}
\providecommand{\s}{\ensuremath{s}}
\providecommand{\sset}{\ensuremath{\mathbf{s}}}
\providecommand{\nsset}{\ensuremath{\mathbf{ns}}}
\providecommand{\n}{\ensuremath{n}}
\providecommand{\nset}{\ensuremath{\mathbf{n}}}
\providecommand{\nnset}{\ensuremath{\mathbf{nn}}}

%Other math
\providecommand{\prob}{\ensuremath{\mathrm{Pr}}}


\begin{document}

% Produce the preamble
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Abstract}
% 1 to 2 paragraphs

Particle swarm optimization (PSO) is an evolutionary algorithm that has proven
effective at solving complicated optimization problems.  In this project we
present a speculative approach to the parallelization of PSO motivated by
speculative execution in modern computer hardware.

In our approach, we refactor PSO such that the computation needed for iteration
$t+1$ can be done in parallel with the computation needed for iteration $t$.
Thus we can perform two iterations of PSO at once.  Even with some amount of
wasted computation, we show that this approach to parallelization in PSO often
outperforms the na\"ive parallelization of simply adding particles to the
swarm.  Our methods produce results that are exactly equivalent to PSO; this is
not a new algorithm or variant, only a new method of parallelization.

However, given this new parallelization model we can relax the requirement of
exactly reproducing PSO in an attempt to produce better results.  We present
several such relaxations, including keeping the best speculative position
evaluated instead of the one corresponding to PSO, and speculating several
iterations ahead instead of just one.  We show that these methods dramatically
improve the performance of parallel PSO in many cases.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
% 1 to 4 pages
\label{sec:intro}

Particle swarm optimization (PSO) has been found to be a highly robust and
effective algorithm for solving many types of optimization
problems~\citep{poli-2008-pso-applications}.  For much of the algorithm's
history, PSO was run serially on a single machine.  However, the world's
computing power is increasingly coming from large clusters of processors.  In
order to efficiently utilize these resources for computationally intensive
problems, PSO needs to run in parallel.

Within the last few years, researchers have begun to recognize the need to
develop parallel implementations of PSO, publishing many papers on the subject.
The methods they have used include various synchronous algorithms
\citep{chu-2006-intelligent-parallel-pso,jin-2005-pso-antenna-designs,%
parsopoulos-2004-parallel-vector-evaluated-pso,%
schutte-2004-parallel-global-optimization-with-pso} and asynchronous algorithms
\citep{mostaghim-2006-multi-objective-pso-on-grids,%
venter-2005-parallel-pso-asynchronous-evaluations}.  Parallelizing the
evaluation of the objective function can also be done in some cases, though
that is not an adaption of the PSO algorithm itself and thus is not the focus
of this project.

These previous parallel techniques distribute the computation needed by the
particles in the swarm over the available processors.  If more processors are
available, these techniques increase the number of particles in the swarm,
either by adding individual particles or by adding entire new sub-swarms.  In
almost all cases, adding additional particles produces better results in the
same amount of time~\citep{mcnabb-2009-large-particle-swarms}.  In
\figref{iters-sphere} we see an example of this on the well-known benchmark
function Sphere (20 dimensions, reporting the average of twenty runs).  In
terms of the number of iterations performed (which is equivalent to wall-clock
time if all particles are evaluated in parallel), every time the swarm size
increases, the performance improves.

\begin{figure}
  \centering
  \includegraphics[width=.4\columnwidth]{iters_sphere}
  \caption{Function Sphere with various swarm sizes, comparing performance with
  the number of iterations of the algorithm performed.}
  \label{fig:iters-sphere}
\end{figure}

However, it can be seen from the graph that once the swarm is sufficiently
large, doubling the swarm size produces only incremental benefit; performance
improves, but not by very much.  The reason for this is that there comes a
point of diminishing returns with respect to adding particles.  In
\figref{evals-sphere} we show the value obtained after 50,000 function
evaluations (not iterations) as a function of swarm size, again for the
function Sphere.  Increasing the swarm size from 5 to 10 has a significant
effect on the value obtained.  However, increasing the swarm size from 16 to 30
makes the algorithm less efficient; that is, it reduces the progress the
algorithm makes per evaluation.  Other functions show similar trends, though
often the optimal swarm size is slightly larger.  For this reason, previous
work has recommended the use of a swarm size of 50 for
PSO~\citep{bratton-2007-defining-a-standard-for-pso}.  Thus, in at least some
cases, adding particles indefinitely will not yield an efficient
implementation.

\begin{figure}
  \centering
  \includegraphics[width=.4\columnwidth]{evals_sphere}
  \caption{Function Sphere with various swarm sizes, comparing performance with
  the number of function evaluations performed.  Error bars show median and
  10th and 90th percentiles.}
  \label{fig:evals-sphere}
\end{figure}

In this project we will consider PSO parallelization strategies for clusters of
hundreds or thousands of processors and functions for which a single evaluation
will take at least tens of seconds, but probably minutes or perhaps hours.  Our
purpose is to explore the question of what to do with a thousand processors
when 50 or 100 particles is the most efficient swarm size, and simply adding
particles results in only incremental improvement.  In our work we assume that
the researcher has some fixed number of processors that should be fully
utilized throughout the optimization process.

In this project we will show that the results of standard PSO can be reproduced
\emph{exactly}, two iterations at a time, using a speculative approach adapted
from speculative execution. We prove that the standard PSO equation can be
factored such that a set of speculative positions can be found which will
always include the position computed in the next iteration.  By computing the
value of the objective function for each of the speculative positions at the
same time the algorithm evaluates the objective function for the current
position, it is possible to know the objective function values for both the
current and the next iteration at the same time.  We will demonstrate this
principle by implementation and show that it produces exactly the same results
as standard PSO, but two iterations at a time.  The resulting implementation
should run efficiently on large clusters where the number of processors is much
larger than a typical or reasonable number of particles, producing better
results in less ``wall-clock'' time.

It is important to note here that this is not a variant of PSO.  We simply
propose a new way to think about the parallelization of PSO that we show
performs better in many cases than previous parallelizations.

Furthermore, we show that if we relax the requirements of the algorithm, no
longer demanding that it strictly reproduce the exact behavior of standard PSO,
we can introduce new speculative techniques that often out-perform na\"ive
parallelizations of PSO.  These relaxations make better use of the information
obtained from the extra exploration made by the speculative function
evaluations.  We also explore the idea that, like branch prediction in
processors, we need not speculatively evaluate \emph{all} possible future
positions, we can accelerate the algorithm even if we are just \emph{likely} to
have guessed right.  By pruning the speculation to just paths that are
statistically likely to reproduce the paths that are equivalent to PSO we can
increase the swarm size without increasing the number of speculative
evaluations.  We also consider several recovery strategies for cases where the
pruned set of speculative evaluations does not contain the evaluation that
standard PSO would have done.  A further improvement we explore is speculating
several iterations ahead instead of just one, which is made possible by pruning
the number of speculative evaluations.

\subsection{Particle Swarm Optimization}
\label{sec:pso}

Particle swarm optimization was proposed in 1995 by James Kennedy and Russell
Eberhart~\citep{kennedy-1995-particle-swarm-optimization}.  The algorithm is
used to intelligently search a multi-dimensional space by mimicking the
swarming and flocking behavior of birds and other animals. It is a social
algorithm that depends on interaction between particles to quickly and
consistently approximate the optimal solution to a given objective function.

The motion of particles through the search space has three components: an
inertial component that gives particles momentum as they move, a cognitive
component where particles remember the best solution they have found and are
attracted back to that place, and a social component by which particles are
attracted to the best solution that any of their neighbors have found.

At each iteration of constricted PSO, the position $\ppos_t$ and velocity
$\pvel_t$ of each particle are updated as follows:
\begin{align}
\label{eq:velupdate}
	\pvel_{t+1} &=
		\constriction \bigl[ \pvel_t
			+ \pcoeff\pURand_{t}\otimes(\pbest_{t} - \ppos_{t}) +
			\ncoeff\nURand_{t}\otimes(\nbest_{t} - \ppos_{t})
		\bigr] \\
\label{eq:posupdate}
	\ppos_{t+1} &= \ppos_{t} + \pvel_{t+1}
\end{align}
where \( \pURand_{t} \) and \( \nURand_{t} \) are vectors of independent random
numbers drawn from a standard uniform distribution, the \( \otimes \) operator
is an element-wise vector multiplication, $\pbest$ (called personal best) is
the best position the current particle has seen, and $\nbest$ (called
neighborhood best) is the best position the neighbors of the current particle
have seen~\citep{bratton-2007-defining-a-standard-for-pso}.  The parameters \(
\ncoeff \), \( \pcoeff \), and \( \constriction \) are given prescribed values
required to ensure convergence (2.05, 2.05, and .73,
respectively)~\citep{clerc-2002-constricted-pso}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related}

There have been several parallel implementations of PSO presented in the
literature.  The improvements described in the literature come in two major
areas: innovations in implementation details and innovations in the use of
topology and swarm size to scale PSO to many processors.

There are many ways to parallelize the basic PSO algorithm.  The most
fundamental decision to make in parallel PSO is which parallel architecture to
use.  Several architectures have been proposed, including Master-Slave, fully
distributed (sometimes called ``diffusion''), and reformulating PSO into
Google's MapReduce framework~\citep{belal-2004-parallel-models-for-pso,
mcnabb-2007-parallel-pso-using-mapreduce}.  A somewhat orthogonal
implementation decision when parallelizing PSO is whether to have synchronous
communication or asynchronous communication.

\subsection{Scaling PSO to many processors}

The other area of research in parallelizing PSO deals not with the
implementation details of architecture and synchronicity, but with what should
be done with the PSO equations when many hundreds or thousands of processors
are available.  The main issues that have been addressed in this space are how
many particles to use for a particular number of processors and what
communication topology should be employed.

The number of particles per processor has typically been decided by how long it
takes to evaluate the function being optimized.  When the function takes longer
than a few seconds to evaluate, previous techniques have assigned the number of
particles in the swarm to be the number of processors available~%
\citep{jin-2005-pso-antenna-designs,mcnabb-2009-large-particle-swarms},
advocating using as many processors as possible to get the best performance.
When the function takes less time to evaluate than the TCP/IP stack used to
send interprocessor communication, parallel implementations assign several or
many particles to a single processor~\citep{chu-2006-intelligent-parallel-pso,
chang-2005-parallel-pso-with-communication-strategies}.  Often the processor
only sends information about the best particle it evaluated to other
processors~\citep{belal-2004-parallel-models-for-pso}.

Another popular method is simply to run PSO independently on each of the
processors available, taking the best result when all of the runs complete.  It
should be noted that this is equivalent to the previously stated method of
assigning many particles to each processor, only with no communication between
processors instead of little communication.  Both of these methods can be
described as changes in the communication topology of the original PSO
algorithm~\citep{mcnabb-2009-large-particle-swarms}.

Thus previous work in parallelizing PSO, apart from implementation details, has
consisted entirely of increasing the swarm size and adapting the topology to be
better suited to parallel computation.

With regard to increasing the swarm size in PSO, some recent work has suggested
that increasing the swarm size throughout the course of the optimization
process provides better results than having a set swarm
size~\citep{hsieh-2009-efficient-population-utilization-for-pso,
montes-de-oca-2010-incremental-social-learning-pso}.  However, these results
focused on serial computation and are based on total number of function
evaluations, which, when running in parallel on expensive functions, is less
important than total number of iterations.  Other work focusing on
parallelization has shown that when extra processors are available they should
be used, as performance increases with swarm size when measuring in terms of
number of iterations~\citep{mcnabb-2009-large-particle-swarms,
jin-2005-pso-antenna-designs}.  If the swarm size were varied throughout the
course of the optimization process, some processors would be sitting idle at
most iterations.

The contribution of our work is in this area of what should be done with the
PSO equations to better utilize a thousand processors when they are available.
In our work we use a synchronous, MapReduce implementation of parallel PSO.
While we use a specific implementation, we describe how speculative evaluation
can be performed in any of the synchronous architectures mentioned in the
previous section.  The adaptation of our methods to asynchronous PSO
parallelization methods should be straightforward, though it is left to future
work.

\section{Thesis Statement}
% 1 to 2 sentences

When many processors are available, a speculative approach to the
parallelization of particle swarm optimization can yield dramatic improvements
in the performance of the algorithm on expensive functions compared to previous
parallelization techniques.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Project Description}
% 2 to 5 pages

PSO can be trivially parallelized by assigning each particle's computation to
an individual processor.  But as we have seen in \figref{evals-sphere}, for
some functions, and for large numbers of processors, just adding particles
reaches a point of diminishing returns.  That is, adding processors does not
help the algorithm reach any given level of fitness appreciably faster.
Instead of adding particles we employ a speculative approach that allows us to
perform two iterations at a time.

Our speculative methods require refactoring the PSO equations such that all
possible positions for each particle at iteration $t+1$ can be evaluated in
parallel along with the position of each particle at iteration $t$.  With some
careful bookkeeping, we can then piece together the results of iteration $t+1$
for each particle, thus using extra processors to evaluate two iterations of
the algorithm in the time it takes to evaluate the function once.  A wise
choice of topology limits the necessary speculative evaluations to seven per
particle.

To see the value of this refactoring, suppose that $1000$ processors are
available, and that the evaluation of the objective function takes one hour.
If we only want a swarm of $100$ particles, $900$ of the processors would be
sitting idle for an hour at every iteration, and it would take two hours to run
two iterations.  If instead we perform speculative evaluation, sending each of
the $7$ possible speculative positions of a particle to be computed at the same
time as its current position, we would use $800$ of the $1000$ processors and
perform two iterations in one hour.

In order to do two iterations at once, we must use 8 times as many processors
as there are particles in the swarm.  If these processors were not performing
speculative evaluation, they might instead be used for function evaluations
needed to support a larger swarm.  This raises the question of whether a swarm
of 100 particles doing twice as many iterations outperforms a swarm of 800
particles.  This project will seek to answer that question.

My honors thesis proposed and developed this basic approach to parallelization
in PSO.  This masters thesis will complete the picture of speculative methods
in particle swarm optimization.  First, we present a mathematical proof of
correctness of our methods as demonstration that what we propose is in fact
mathematically equivalent to standard parallelization methods in PSO.  Second,
we propose pruning the set of possible speculative evaluations to better use
available processors.  We motivate the pruning by analyzing the statistics of
which branches are commonly taken in PSO, using those statistics to prune all
but the most likely branches.  Further, once the idea of pruning is
established, we view the set of all possible speculative evaluations as an
infinite tree, not confining ourselves to speculating only about the next
iteration.  We then experiment with speculating down likely paths three or four
iterations ahead, and show that this by far outperforms simple one-iteration
speculative evaluation.  And lastly, as my honors thesis used only a few
experiments on three benchmark functions, we give a better picture of the
performance of speculative approaches with much more extensive experimentation.

\subsection{Proof}

Some readers of my honors thesis had a hard time understanding our methods,
thinking that we were changing the PSO algorithm when performing speculative
evaluation.  To alleviate any such concerns and provide a strong theoretical
background for speculative approaches to parallelization in PSO, we will
provide a mathematical proof that PSO can be decomposed in such a way as to
exactly reproduce the behavior of the original algorithm.

\subsection{Statistically Motivated Pruning}

The key idea in this proposal is that of decreasing the number of speculative
evaluations needing to be done by pruning away insignificant branches from the
set of all possible evaluations.  In my honors thesis I found that exactly
reproducing PSO two iterations at a time required too many extra evaluations to
give competitive results in most cases.  To fix this problem, I will find ways
to still speculate about future positions without actually evaluating all of
the possible options.  This should improve the performance of the algorithm.

Pruning the set of speculative evaluations necessitates a mechanism for
deciding which branches are desirable.  A na\"ive approach to pruning would be
to try the last branch taken by each particle.  Preliminary experimentation
with this method has shown it to be ineffective.  A smarter approach would be
to examine the statistical behavior of the branches that PSO takes and use that
analysis as motivation for deciding which branches to prune.  We will collect
statistics on the branching behavior of PSO across several functions and
topologies.  We will then use those statistics to perform intelligent pruning
of the set of speculative evaluations.

\subsection{Speculating Several Iterations Ahead}

Speculating one iteration ahead proved very costly in terms of the number of
extra evaluations required.  Speculating two iterations ahead would require the
square of the number of extra evaluations of speculating a single iteration
ahead.  While exactly speculating two iterations ahead would be far too costly
to be practical, when combined with statistical pruning we can experiment with
speculating along statistically promising lines several iterations ahead.
Preliminary experimentation shows that this can be incredibly effective,
improving the time to completion by a factor of 2 or more, though it also seems
prone to premature convergence.  This thesis will further explore the idea of
speculating several iterations ahead.

\section{Validation}
% 1/2 to 2 pages

This project is expected to produce an implementation of speculative evaluation
in PSO, including several variations on the speculative evaluations performed.
The project will also show that speculative approaches to parallelization in
PSO result in improved performance over non-speculative approaches.

To show improved performance, we will test our methods against previous
parallelization methods on several benchmark functions.  While benchmark
functions themselves take fractions of a second to evaluate and thus have no
need of the kind of parallelization we explore in this project, they also have
no need of particle swarm optimization at all, as they can be solved
analytically.  Yet they are useful to us and to optimization researchers
generally because they stand as surrogates for the kinds of functions
practitioners are actually interested in, and they allow us to explore the
behavior of optimization algorithms on several different classes of functions
in an easy and standardized way.

We will measure our performance on these benchmark functions in terms of the
best value found at each iteration of the algorithms.  We will also compare the
total number of iterations required by each algorithm to reach some specified
value.  The project will be successful if a statistically significant decrease
in the number of iterations is observed.

For each function, we will use the topology that is generally considered to be
best for optimizing that particular function, as described by Bratton and
Kennedy~\cite{bratton-2007-defining-a-standard-for-pso}.  When the Complete
topology is considered best, we will instead use a Random topology that we
developed that mimics the behavior of Complete while being more suited for
parallel computation~\cite{mcnabb-2009-large-particle-swarms}.

The functions we will use in our comparisons are these common benchmark
functions: Bohachevsky, Griewank, Rastrigin, Sphere, and Schwefel 2.21.  We
will run each function with 20, 50, and 500 dimensions to test the scalability
of our methods up to higher dimensions.  We will also test our methods on a
more typical ``real-world'' problem, that of fitting a model to a large amount
of data.  While we will run tests on a complex crystallography problem in
collaboration in Dr. Branton Campbell in the physics department, we will
present our results on a similar but more well known problem.  In particular,
we will train a radial basis function on a set of 10,000 data points.  This is
a similar problem to Dr. Campbell's, in that both seek to match model
parameters to observed data, but it is easier to describe and quicker to
experiment with.  Dr. Campbell also has yet to publish his results and we do
not wish to publish about his problem before he does.

We will compare our speculative methods to previous parallelization techniques,
including a naive parallelization of just increasing the swarm size, and a
subswarms implementation that uses several smaller, independent swarms.

\section{Thesis Schedule}
% 1/4 to 1/2 page

\begin{itemize}

\item Completed: Implement speculative evaluation in PSO and several
  improvements on it.

\item May 13, 2011: Finish running experiments and start incorporating the
  results into the thesis.
  
\item May 30, 2011: Give final draft of thesis to advisor for review.

\item June 10, 2011: submit thesis to second committee member for review.

\item June 30, 2011: submit thesis to third committee member for review.

\item July 15, 2011: defend thesis.

\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\renewcommand\bibsection{\section{Annotated Bibliography}}
%\newcommand\bibpreamble{Material to be added between heading and entries.}
% 2 to 5 pages

\bibliography{bib}


\end{document}

\documentclass{article}
\usepackage{nips10submit_e,times}
\usepackage{graphicx}
\usepackage{amsmath}

% Better references, I think
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}

\title{Better Corpus Visualization through Topic Ranking}
\author{
Matthew J. Gardner \\
Department of Computer Science \\
Brigham Young University \\
Provo, UT 84604 \\
\texttt{mjg82@byu.edu} \\
\And
Other authers\ldots \\
Affiliation \\
Address \\
\texttt{email} \\
}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}
\maketitle

\begin{abstract}

Topic modeling is a popular and effective method for gathering meaning from a
set of documents.  Such models contain a lot of information and can be very
difficult to analyze.  For the proper visualization of these models, one must
present only the topics that are relevant to the task at hand.  We present the
novel task of topic ranking in response to user questions that greatly aids in
the visualization of topic models.  We make use of the document distribution of
a topic in order to rank the topics according to specific user questions.

\end{abstract}

\section{Introdcution}
\label{sec:introduction}

Topic modeling has been shown to be an effective means of extracting semantic
meaning from a collection of
documents~\cite{blei-2003-latent-dirichlet-allocation}.  Topic models have been
used as an inputs to other kinds of algorithms, such as classification and
sentiment tasks (cite some papers here), but the usefulness of topic models as
visualization tools in and of themselves has not been well established, and is
even in question (cite more papers).  Some recent work has been done in an
attempt to make corpus browsers using the results of LDA, and to make the
results of LDA more easily interpretable, but there is much work yet to be done
(still more citations).  

Common approaches to using topic models to facilitate corpus browsing and
visualization are limited mainly to showing the similarity between documents in
the corpus in various ways.  Such methods are useful in their own right when
one is individually scanning documents, but they are insufficient to answer
broader questions about the corpus, like, ``What topics show up most in these
documents?'' and, ``What does a particular author in the corpus talk about most
frequently that is unique to that author?''  Those questions turn one away from
analyzing individual documents to trying to get information out of the topics
themselves.

Upon looking at the topics one is confronted with tens or hundreds of lists of
words, sorted by a number given to each list by the algorithm.  Sifting through
those lists and trying to understand what they mean, and whether or not a
particular topic is important, can be a daunting task, particularly to one not
very familiar with the algorithm.  The common practice of cherry-picking good
topics to present in a paper hides the fact that a large number of topics
returned by topic models are very hard to interpret or even completely
meaningless to humans.

In order for topic models to be useful for corpus browsing, some sense has to
be made out of the topics.  One recently proposed method is to assign to each
topic a coherence value, so that the person can ignore topics that are likely
to be meaningless~\cite{newman-2010-automatic-evaluation-of-topic-coherence}.
While topic coherence is able to eliminate meaningless topics from
consideration, it is not able to address many broader questions that a person
browsing the corpus may have.

In this paper we present the novel task of ranking topics by relevance to
specific user questions.  If one is to successfully visualize a corpus in a way
that answers broad questions, and if topics, not documents, are the natural
answer to those questions, then one must properly rank topics such that the
most prominent topics contain the answer to the user's questions.  We recognize
that not all questions about a corpus fit into this visualization model, but
there are many that do, such as those listed above.

The rest of this paper is outlined as follows.  We first give further
motivation and a more detailed description of the task of ranking topics by
relevance in \secref{ranking}.  \secref{docdist} describes a method for
determining the relevance of a topic to various possible questions a person
could have when looking at a corpus, and \secref{examples} shows examples of
how this technique provides a proper ranking.  Finally, in \secref{conclusion}
we conclude.

\section{The topic ranking task}
\label{sec:ranking}

We assert that for many tasks, the proper visualization of a topic model is a
ranking of topics, not simply a window onto individual documents.  When one
wants to see what topics are prevelant in a corpus, the answer is not found by
scanning individual documents, but by looking at the topics produced by a topic
model.  Likewise, when one wants to see trends of topics over time, one does
not look through the documents, but at the topics, computing some statistics of
those topics for each year for which there are documents.  Thus a proper
visualization of the topic model for these tasks is to show most prominently
those topics that are most relevant to the task.

We can formalize the topic ranking problem simply as follows.  Given a set of
topics from a topic model of a corpus, and a user question about the corpus,
rank the topics by relevance to the user's question.  This is very similar to
basic information retrieval tasks, but also significantly different, in that we
are not looking for a ranking of documents as the result of a query, but a
ranking of topics which answer broader questions about the corpus as a whole.

Clearly the topic ranking task is fundamental to the proper visualization of
topic models, but as far as we are aware, this is the first attempt to even
consider the problem.  We propose a very simple way to answer some user
questions through the use of the document distribution of each topic.  This
technique is by no means a general solution to the problem, but for some
specific tasks it provides rankings that gives users the topics that answer
their questions.

\section{The document distribution for a topic}
\label{sec:docdist}

The distribution of topics in a document, $p(z|d)$, has been widely used in all
applications of topic modeling, from dimensionality reduction of document
representations to showing similar documents in a corpus browser.  This is
understandable, given that estimating the distribution $p(z|d)$ is one of the
primary purposes of topic modeling.  However, $p(d|z)$, the document
distribution for a given topic, also contains interesting information that can
be very useful for visualizing and understanding the output of the topic model.
We use various aspects of this distribution as approximations of relevance to
certain user questions.

The document distribution for a topic can be thought of as follows: suppose we
have a token labeled with a given topic; what is the probability distribution
over documents in the corpus that this token could have come from?  We
approximate this distribution with a sample from a Gibbs sampler of LDA.  The
distribution is thus defined as
\begin{align}
  \label{eq:entropy}
  p(d|z) = \frac{C(d,z)}{\sum_{d \in \mathcal{D}} C(d,z)}
\end{align}
where $d$ is a particular document, $z$ is a particular topic, $C(d,z)$ is the
number of tokens in document $d$ labeled with topic $z$, and $\mathcal{D}$ is
the corpus.

Our use of the document distribution for a topic focuses primarily on
calculating the entropy of the distribution and of related distributions where
documents are grouped by category.  For instance, one can group the documents
by author and compute the distribution of the topic over authors as
\begin{align}
  \label{eq:attrentropy}
  p(a|z) = \frac{\sum_{d \in \mathcal{A}} C(d,z)}{\sum_{d \in \mathcal{D}}
  C(d,z)}
\end{align}
where $a$ is an author and $\mathcal{A}$ is the set of documents written by
that author.

Also, one can compute the distribution over any subset of the documents, where
$p(d|z)$ is defined as in \eqref{eq:entropy}, but instead of the entire corpus,
$\mathcal{D}$ represents some specific subset.  When using subsets of the
documents, we only consider topics which have at least one token in the given
subset, so as to avoid division by zero.

\section{Example topic rankings}
\label{sec:examples}

We present here a few examples of questions a user might ask about a corpus and
how we can rank topics according to their relevance to those questions.  In our
examples we use a corpus of 480 campaign speeches from the 2008 presidential
primary and general election.  We inferred 150 topics in the data from a Gibbs
sampling of the LDA topic model.

The first question we address is, What topics are most prevelant in the corpus?
This is a simple question with an obvious answer that does not require using
$p(d|z)$.  One can simply order the topics by the number of tokens in the
topic.  Upon doing so one finds the following topics (we show the topic number
and the top ten words in the topic for the top five topics):

\begin{tabular}{l|l}
  35 & years time made long today place home day ago back \\
  42 & people don ve make country ll time put didn money \\
  119 & make national work effort including protect adiministration clear
	support policy \\
  15 & america world american future challenges great lead generation nation
	leadership \\
  108 & president work day senator house make white clinton care support \\
\end{tabular}

One can see that these topics for the most part are meaningless, capturing stop
words and noise in the data and hardly answering the question at hand.  When we
filter by a minimum value of coherence, however, we see a better picture:

\begin{tabular}{l|l}
  42 & people don ve make country ll time put didn money \\
  119 & make national work effort including protect adiministration clear
	support policy \\
  120 & law government rights respect case society fact laws principles order \\
  143 & jobs ve create economy make years billion companies invest energy \\
  101 & children family day man mother care love father parents young \\
\end{tabular}

While still somewhat noisy, this list gives the viewer some idea of what was
spoken of most during the 2008 presidential election.

Next we look at a similar question: What topics did John McCain talk about the
most?  This question is also answered without needing to use $p(d|z)$; simply
order the topics by the number of tokens that occur in speeches given by
McCain.  The list is the following (here again we filter by coherence):

\begin{tabular}{l|l}
  129 & mccain news john senator obama remarks america events donate email \\
  119 & make national work effort including protect adiministration clear
	support policy \\
  27 & mccain news john search media speeches www johnmccain email freedom \\
  90 & war iraq honor defeat enemy greater responsibility responsibilities
	difficult great \\
  66 & dollars reform federal washington congress reforms public policy
	problems system \\
\end{tabular}

Another similar, though slightly distinct, question is, What topics did Barack
Obama speak of most consistently throughout his campaign?  This question
differs from the previous in that the previous question could be answered by a
topic Obama used just a few times in long speeches.  This question looks for
topics that were consistent in all of his speeches.  Here we turn to the
document entropy for each topic as in \eqref{eq:entropy}, looking only at
speeches given by Obama.  The result of such an ordering is:

\begin{tabular}{l|l}
  60 &  barack remarks senator barackobama obama ll www http hope states\\
  47 & mccain senator john ll bush washington states election won george \\
  124 & change politics country time american washington ve common americans
	end \\
  84 & delegates south future party carolina kennedy support pledged total
	versus \\
  143 & jobs ve create economy make years billion companies invest energy \\
\end{tabular}

The first topic consists of markup on all of Barack Obama's speeches.  From the
rest of the topics one can see that Obama spoke a lot about his opponent,
Senator McCain, about change and the economy, and he apparently spent a lot of
time in South Carolina.

Another question one may have about these speeches is, What topics are most
consistent across candidates?  Each candidate has issues particular to them,
but what topics are universally used by the candidates?  A natural ranking for
this question is to order by entropy over candidates, as in
\eqref{eq:attrentropy}.  Such a ranking shows these to be universal topics:

\begin{tabular}{l|l}
  8 & congress government federal bill legislation members house conference
	statement congressional \\
  104 & states united countries international support economic state force
	military community \\
  103 & public political understand values home play rules shared committed
	important \\
  93 & president bush administration george war failed stop department
	september moral \\
  18 & result led group death allowed consequences early continued quickly
	balance \\
\end{tabular}

Similarly, one could ask, What topics were constant throughout the campaign?
This question is also answered with \eqref{eq:attrentropy}, with the month of
the speech used to group the documents together.  We see these topics as a
result:

\begin{tabular}{l|l}
  120 & law government rights respect case society fact laws principles order \\
  8 & congress government federal bill legislation members house conference
	statement congressional \\
  25 & party vote election campaign votes people democrats state democratic
	candidate \\
  101 & children family day man mother care love father parents young \\
  38 & economy economic american world security prosperity trade market markets
	growth \\
\end{tabular}

\section{Conclusion}
\label{sec:conclusion}

Displaying the right topics to a user is essential to the effective
visualization of topic models.  We have presented the novel task of ranking
topics according to relevance to certain user questions.  We have also given a
method for ranking topics in order to answer a few specific questions.  Such
rankings allow visualizations of topic models that present information to users
in much more effective ways than was previously possible.

While our methods are useful for a few user questions, it remains as future
work to devise a method for ranking topics in answer to arbitrary questions.
The evaluation of such rankings is also a difficult task that must be left to
future work.  

\bibliographystyle{unsrt}
\bibliography{../../../bib/lda/bib}

\end{document}

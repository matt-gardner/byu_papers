\documentclass{article}
\usepackage{nips10submit_e,times}
\usepackage{graphicx}
\usepackage{amsmath}

% Better references, I think
\newcommand{\secref}[1]{Section~\ref{sec:#1}}
\newcommand{\figref}[1]{Figure~\ref{fig:#1}}
\newcommand{\algref}[1]{Algorithm~\ref{alg:#1}}

\title{Better Corpus Visualization through Topic Ranking}
\author{
Matthew Gardner \\
Department of Computer Science \\
Brigham Young University \\
\texttt{mjg82@byu.edu} \\
\And
Kevin Seppi \\
Department of Computer Science \\
Brigham Young University \\
\texttt{kseppi@cs.byu.edu} \\
\And
Eric Ringger \\
Department of Computer Science \\
Brigham Young University \\
\texttt{ringger@cs.byu.edu} \\
}


\begin{document}
\nipsfinalcopy % Uncomment for camera-ready version
\maketitle

\begin{abstract}

Topic modeling is a popular and effective method for gathering meaning from a
set of documents.  Such models contain a lot of information and can be very
difficult to analyze.  Current attempts at using topic models to enhance
information discovery in a document collection are limited mainly to simple
corpus browsers.  However, there are many questions a person may have about a
corpus that are best answered by the topics found by the topic model.  To get
these kinds of information from the model, one must decide which topics best
answer the question and present only the best topics.  In an attempt to address
this issue, we present the novel task of topic ranking in response to these
kinds of user questions and give some methods that are useful in this task.

\end{abstract}

\section{Introduction}
\label{sec:introduction}

There are many kinds of questions a person might have when confronted with a
previously unseen collection of documents.  Some of those questions are
factual, such as, ``Who was Barack Obama's campaign advisor?''  Some questions
look for documents, such as ``What speeches did John McCain give about the war
in Iraq?''  Still other questions ask about topics and topical trends in the
corpus, such as, ``What were Obama's consistent campaign themes?''  Each kind
of question requires a separate kind of visualization.  The first two kinds of
questions listed require facts and documents that answer the question, and the
methods of information retrieval are well suited to those questions.  The last
kind of question, however, naturally falls into the domain of topic modeling.


Topic modeling has been shown to be an effective means of extracting semantic
meaning from a collection of
documents~\cite{griffiths-2004-finding-scientific-topics}.  Topic models have
been used as an inputs to other kinds of algorithms, such as classification and
sentiment tasks~\cite{blei-2003-latent-dirichlet-allocation,
brody-2010-aspect-sentiment-model-for-reviews}, but, as far as we are aware,
their use in revealing information about a corpus has previously been limited
to corpus browsing~\cite{newman-2010-visualizing-with-topic-maps,
blei-arxiv-corpus-browser}.  While topic-based corpus browsers give a better
window into the documents in the corpus and can help to visualize some kinds of
information, they are not capable of effectively answering questions about the
topics in the corpus.

In order to properly visualize a topic model in response to topical questions,
one must present information about the topics themselves, not the documents,
and decide which topics are most relevant to the information desired by the
user.  Typical methods of getting such information from a topic model require
the user to sift through tens or hundreds of lists of words, trying to
understand what they mean, and decide whether or not a particular topic is
relevant.  This can be a daunting task.  The common practice of hand-picking
good topics to present in a paper hides the fact that a large number of topics
returned by topic models are very hard to interpret or even completely
meaningless to humans.

Clearly, then, for topic models to be useful for visualizing corpora in
response to these kinds of questions, some sense has to be made out of the
topics.  One recently proposed method is to assign to each topic a coherence
value, so that the person can ignore topics that are likely to be
meaningless~\cite{newman-2010-automatic-evaluation-of-topic-coherence}.  While
topic coherence is able to eliminate meaningless topics from consideration, it
is not able to address many broader questions that a person browsing the corpus
may have.

In an attempt to address this issue, we present the novel task of topic
ranking; that is, deciding which topics are most pertinent to a user's
topically-based question.  In general this is a very difficult problem, but it
is essential to the proper visualization of topic models for many applications.
Surprisingly, it appears that researchers have not as yet attempted to solve
this problem.  In this paper we formalize the topic ranking problem and present
some initial attempts at solving a limited subset of possible questions.

The rest of this paper is outlined as follows.  We first give further
motivation and a more detailed description of the topic ranking task in
\secref{ranking}.  \secref{docdist} describes a method for determining the
relevance of a topic to a few types of topically-based questions a person could
have when looking at a corpus, and \secref{examples} shows examples of how this
technique provides a proper ranking.  Finally, in \secref{conclusion} we
conclude.

\section{The topic ranking task}
\label{sec:ranking}

We assert that for many tasks, the proper visualization of a topic model is a
ranking of topics, not simply a window onto individual documents.  When one
wants to see what topics are prevalent in a corpus, the answer is not found by
scanning individual documents, but by looking at the topics produced by a topic
model.  Likewise, when one wants to see trends of topics over time, one does
not look through the documents, but at the topics, computing some statistics of
those topics for each year for which there are documents.  Thus the correct
visualization of a topic model for these tasks includes showing most
prominently those topics that are most relevant to the task.

We can formalize the topic ranking problem simply as follows.  Given a set of
topics from a topic model of a corpus, and a topically-based question about the
corpus (that is, a question whose answer is naturally a topic, not a fact or a
document), rank the topics by relevance to the question.  This is similar to
some information retrieval tasks, but also significantly different, in that we
are not looking for a ranking of documents as the result of a query, but a
ranking of topics which answer broader questions about the corpus as a whole.

Clearly the topic ranking task is fundamental to the proper visualization of
topic models, but as far as we are aware, this is the first attempt to even
consider the problem.  We propose a very simple way to answer a few types of
topically-based questions through the use of the document distribution of each
topic.  This technique is by no means a general solution to the problem, but we
show it to be useful for visualizing at least a subset of the kinds of
information one might be interested in obtaining from a corpus.

\section{The document distribution for a topic}
\label{sec:docdist}

The distribution of topics in a document, $p(z|d)$, has been widely used in all
applications of topic modeling, from dimensionality reduction of document
representations to showing similar documents in a corpus browser.  This is
understandable, given that estimating the distribution $p(z|d)$ is one of the
primary purposes of topic modeling.  However, $p(d|z)$, the document
distribution for a given topic, also contains interesting information that can
be very useful for visualizing and understanding the output of the topic model.
We use various aspects of this distribution as approximations of relevance to
certain user questions.

The document distribution for a topic can be thought of as follows: suppose we
have a token labeled with a given topic; what is the probability distribution
over documents in the corpus that this token could have come from?  We
approximate this distribution with a sample from a Gibbs sampler of LDA.  Our
approximation of this distribution is thus defined as
\begin{align}
  \label{eq:entropy}
  p(d|z) = \frac{C(d,z)}{\sum_{d \in \mathcal{D}} C(d,z)}
\end{align}
where $d$ is a particular document, $z$ is a particular topic, $C(d,z)$ is the
number of tokens in document $d$ labeled with topic $z$, and $\mathcal{D}$ is
the corpus.

Our use of the document distribution for a topic focuses primarily on
calculating the entropy of the distribution and of related distributions where
documents are grouped by category.  For instance, one can group the documents
by author and compute the distribution of the topic over authors as
\begin{align}
  \label{eq:attrentropy}
  p(a|z) = \frac{\sum_{d \in \mathcal{A}} C(d,z)}{\sum_{d \in \mathcal{D}}
  C(d,z)}
\end{align}
where $a$ is an author and $\mathcal{A}$ is the set of documents written by
that author.

Also, one can compute the distribution over any subset of the documents, where
$p(d|z)$ is defined as in \eqref{eq:entropy}, but instead of the entire corpus,
$\mathcal{D}$ represents some specific subset (in our applications the subset
always corresponds to a set of documents having some particular attribute, such
as a common author).  When using subsets of the documents, we only consider
topics which have at least one token in the given subset, so as to avoid
division by zero.

\section{Example topic rankings}
\label{sec:examples}

We present here a few examples of questions a user might ask about a corpus and
how we can rank topics according to their relevance to those questions.  In our
examples we use a corpus of 480 campaign speeches from the 2008 presidential
primary and general election.  We inferred 150 topics in the data from a Gibbs
sampling of the LDA topic model.

The first question we address is, What topics are most prevalent in the corpus?
This is a simple question with an obvious answer that does not require using
$p(d|z)$.  One can simply order the topics by the number of tokens in the
topic.  Upon doing so one finds the following topics (we show the topic number
and the top ten words in the topic for the top five topics):

\begin{tabular}{l|l}
  35 & years time made long today place home day ago back \\
  42 & people don ve make country ll time put didn money \\
  119 & make national work effort including protect administration clear
	support policy \\
  15 & america world american future challenges great lead generation nation
	leadership \\
  108 & president work day senator house make white clinton care support \\
\end{tabular}

One can see that these topics for the most part are meaningless, capturing stop
words and noise in the data and hardly answering the question at hand.  When we
filter by a minimum value of coherence, however, we see a better picture:

\begin{tabular}{l|l}
  42 & people don ve make country ll time put didn money \\
  119 & make national work effort including protect administration clear
	support policy \\
  120 & law government rights respect case society fact laws principles order \\
  143 & jobs ve create economy make years billion companies invest energy \\
  101 & children family day man mother care love father parents young \\
\end{tabular}

While still somewhat noisy, this list gives the viewer some idea of what was
spoken of most during the 2008 presidential election.

Next we look at a similar question: What topics did John McCain talk about the
most?  This question is also answered without needing to use $p(d|z)$; simply
order the topics by the number of tokens that occur in speeches given by
McCain.  The list is the following (here again we filter by coherence):

\begin{tabular}{l|l}
  129 & mccain news john senator obama remarks america events donate email \\
  119 & make national work effort including protect administration clear
	support policy \\
  27 & mccain news john search media speeches www johnmccain email freedom \\
  90 & war iraq honor defeat enemy greater responsibility responsibilities
	difficult great \\
  66 & dollars reform federal washington congress reforms public policy
	problems system \\
\end{tabular}

Another similar, though slightly distinct, question is, What topics did Barack
Obama speak of most consistently throughout his campaign?  This question
differs from the previous in that the previous question could be answered by a
topic Obama used just a few times in long speeches.  This question looks for
topics that were consistent in all of his speeches.  Here we turn to the
document entropy for each topic as in \eqref{eq:entropy}, looking only at
speeches given by Obama.  The result of such an ordering is:

\begin{tabular}{l|l}
  60 &  barack remarks senator barackobama obama ll www http hope states\\
  47 & mccain senator john ll bush washington states election won george \\
  124 & change politics country time american washington ve common americans
	end \\
  84 & delegates south future party carolina kennedy support pledged total
	versus \\
  143 & jobs ve create economy make years billion companies invest energy \\
\end{tabular}

The first topic consists of markup on all of Barack Obama's speeches.  From the
rest of the topics one can see that Obama spoke a lot about his opponent,
Senator McCain, about change and the economy, and he apparently spent a lot of
time in South Carolina.

Another question one may have about these speeches is, What topics are most
consistent across candidates?  Each candidate has issues particular to them,
but what topics are universally used by the candidates?  A natural ranking for
this question is to order by entropy over candidates, as in
\eqref{eq:attrentropy}.  Such a ranking shows these to be universal topics:

\begin{tabular}{l|l}
  8 & congress government federal bill legislation members house conference
	statement congressional \\
  104 & states united countries international support economic state force
	military community \\
  103 & public political understand values home play rules shared committed
	important \\
  93 & president bush administration george war failed stop department
	september moral \\
  18 & result led group death allowed consequences early continued quickly
	balance \\
\end{tabular}

Similarly, one could ask, What topics were constant throughout the campaign?
This question is also answered with \eqref{eq:attrentropy}, with the month of
the speech used to group the documents together.  We see these topics as a
result:

\begin{tabular}{l|l}
  120 & law government rights respect case society fact laws principles order \\
  8 & congress government federal bill legislation members house conference
	statement congressional \\
  25 & party vote election campaign votes people democrats state democratic
	candidate \\
  101 & children family day man mother care love father parents young \\
  38 & economy economic american world security prosperity trade market markets
	growth \\
\end{tabular}

\section{Conclusion}
\label{sec:conclusion}

Many kinds of information a person might desire to get from a collection of
documents are best presented as topics from a topic model.  In order to
determine the relevance of each topic to specific user questions, we have
presented the novel task of topic ranking.  We have presented methods to answer
questions of the form, ``What are the consistent themes in this corpus, or for
this author?''  We have also shown results on a corpus of political speeches
that intuitively seem correct.  

We believe that the topic ranking task is an important problem that deserves
more attention than it has as yet received.  This work is very preliminary in
that it only addresses a very small subset of possible topically-based
questions.  We plan on continuing to explore topic ranking, expanding the set
of questions that can be answered and searching for automated means of
evaluating these rankings.

\bibliographystyle{unsrt}
\bibliography{../../../bib/lda/bib}

\end{document}

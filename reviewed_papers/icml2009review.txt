The English in this paper is distracting and in some places almost unreadable. The algorithms presented should be presented in psuedocode, instead of the incredibly distracting "Step" notation, and when symbols are used they are not used clearly (particularly, when you use a "for all" symbol, pull it in front of the part where it is used: "[For all] s, a: set P_i(s,a) to be Q_i(s,a)", for example).

The biggest problem with this paper is that it's not novel. Particle swarm optimization has been used in reinforcement learning before, and the lifespan technique presented in this paper was presented long ago in the PSO literature to solve problems in dynamic environments, and no credit is given nor comparisons offered to previous ideas. The authors claim that "because the personal best and the global best of PSO for optimization problems are determined based on objective function values, to introduce the lifespan is not necessary." In fact, that is a well studied area very prevelant in PSO literature, and many techniques have been developed to account for noisy and dynamic objective functions. Just a few of the papers that should be cited are found at the bottom of this review. See the papers they cite and the papers that cited them for more.

The second biggest problem with this paper is that it's not noteworthy. The original SRL-PSO might have been noteworthy, except it is only compared to Q-learning, a 30-year-old algorithm that hundreds of people have beaten already, on a problem that isn't very hard or very interesting. This technique should be compared to something more state-of-the-art, on a more difficult problem, or at least a variety of problems, to show consistent performance. It might be wise to try a function for which Q-learning works well in the first place - large spaces with single point rewards were never very good for Q-learning. But all this paper presents is a variant of SRL-PSO that I'm not convinced is statistically better than the original (and is a variant of PSO that has long since been proposed). The graphs presented are very close, and there is no statistical comparison presented to show convincingly that the proposed algorithm has any real improvement. Overall, it doesn't seem like big news to me.

My next problem with this paper is that the results aren't repeatable. Exactly what was done is not explained well enough that I could code it up and see the same results. Specifically, the four different cases presented for the shortest path problem are not specific enough - how many walls are there, and how are they generated? What's the termination condition in your algorithm? And secondly, in the evidence presented that SRL-PSO performs poorly when it has long-lived personal bests, the method is unclear. In "Step 2" of the confirmation described, it says "Re-calculate evaluated values E 300 times for the long-lived personal best, and determine the maximum E_max of these evaluated values." Does that mean to do 300 episodes with the same Q-values, and report the best evaluation found? The results seem strange to me, if that's the case. How could the best found after 300 trials be worse by an average of 400 steps than the one run that you made during the learning process (because the E you're using is essentially the number of steps that were taken to achieve the goal)? Obviously, the over-valued Q-values were a fluke based on a number of random choices. That brings me to my next point - the evaluation function for Q-values.

With your reward structure, the discount parameter d in your evaluation (equation 2) does absolutely nothing. Each episode has a certain number of steps taken to reach the reward. All episodes with the same number of steps have the same total reward, as the reward is -1 for all states except the reward state. So you have a reward of 100 preceded by a string of -1's. The number of steps determines the evaluation function, and the discount means nothing. Clearly it is not doing what you think it does when you say "the rewards obtained by using the Q-values at and shortly after the beginning are considered to have only a little relation with the Q-values at the end." If you had tried the algorithm on a wider variety of problems, perhaps you would have encountered a problem where the d actually does something, but it doesn't with this reward structure. But even if it did, I'm not convinced that it's the right thing to do, because when a state-action pair was seen during an episode doesn't tell you how many times it was updated during the episode. And given the observation in the preceding paragraph, and in your paper, the evaluation function should probably include a weighting for how many random actions were taken during the episode, as those actions don't have any relation to the Q-values that were in use at the time. That might solve your problems better than using a variant of PSO.

And lastly, the graphs you present in section 4.3 seem questionable to me. You say that flukes happen, and you sometimes find quick paths to the end with bad Q-values, but you report in the graph the minimum number of actions found in an episode so far. What if the episode that got you that minimum number of episodes was a fluke? That does not reflect the current "value" (for lack of a better word) of your set of Q-values. Your entire stated reason for presenting this variant of SRL-PSO was to overcome the problem of abnormally short episodes that produced over-valued Q-values, yet when reporting your results you treat those short episodes as a good thing, and keep them around. For all I know from what you reported, average performance from SRL-PSO based on the current Q-values at each iteration might be right up with regular Q-learning. Average number of steps to the end using those Q-values might be more meaningful, as you wouldn't hold on to and report lucky results from previous iterations. And incidentally, you have four times the number of agents in SRL-PSO, so you have four times the chance of getting lucky and keeping around an abnormally short episode. I'm not convinced at all that the results as presented are meaningful.

Some papers that should be cited:

Pugh, Zhang, and Martinoli, "Particle Swarm Optimization for Unsupervised Robotic Learning," Swarm Intelligence Symposium 2005.

Pugh and Martinoli, "Multi-robot learning with particle swarm optimization," International Conference on Autonomous Agents, 2006.

Carlisle and Dozier, "Adapting Particle Swarm Optimization to Dynamic Environments," International Conference on Artificial Intelligence, 2000.

Mullen et al, "Particle Swarm Optimization in Dynamic Pricing," CEC 2006.

And there are many more that present similar ideas. 
